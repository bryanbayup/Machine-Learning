{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPItOH6ZrmV68MGuDtQEKvV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanbayup/Machine-Learning/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalasi Library Tambahan\n",
        "!pip install gensim\n",
        "!pip install keras-tuner --upgrade\n",
        "!pip install imbalanced-learn\n",
        "!pip install Sastrawi\n",
        "!pip install sentencepiece\n",
        "!pip install seqeval\n",
        "!pip install rapidfuzz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy1FZwE59O-q",
        "outputId": "8a2cca65-ec2c-4263-e17c-4fa151105251"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=0af6651cc963d06f90bbf76b302078142628980612ac592a39a401d065f3303e\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library yang Dibutuhkan\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "import random\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from keras_tuner import HyperModel, RandomSearch\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import sentencepiece as spm\n",
        "from nltk.corpus import stopwords\n",
        "from seqeval.metrics import classification_report as seq_classification_report\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from rapidfuzz import process, fuzz\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHUP4uFD9S9S",
        "outputId": "e2984ace-840a-4c2b-82ae-bd816f0ff175"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Download FastText\n",
        "!wget -O id.tar.gz \"https://www.dropbox.com/scl/fi/sju4o3keikox69euw51vy/id.tar.gz?rlkey=5jr3ijtbdwfahq7xcgig28qvy&e=1&st=gntzkzeo&dl=1\"\n",
        "!tar -xzf id.tar.gz\n",
        "\n",
        "# Load FastText\n",
        "try:\n",
        "    fasttext_model = KeyedVectors.load_word2vec_format('id.vec', binary=False)\n",
        "    print(\"FastText 'id.vec' berhasil dimuat.\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal memuat 'id.vec': {e}\")\n",
        "    raise ValueError(\"Gagal memuat FastText.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT5Om1un9WR9",
        "outputId": "d5ae2385-b470-4800-fcba-a060e0c21d03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-30 10:23:10--  https://www.dropbox.com/scl/fi/sju4o3keikox69euw51vy/id.tar.gz?rlkey=5jr3ijtbdwfahq7xcgig28qvy&e=1&st=gntzkzeo&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6022:18::a27d:4212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc731f6b38199ce8d7d0794c5e85.dl.dropboxusercontent.com/cd/0/inline/CfWhJRJZjR7JDyB-y8uZol1GztwgkwxY62Ga4F61-rcOvbLZjaXRhIXUck86i7vITQ8yyPVUT3-VkuVteQbdXaXicftb-t4PwWRvnTfJBSc8QVl0zA6ltqPgrEckejJfjuw/file?dl=1# [following]\n",
            "--2024-11-30 10:23:11--  https://uc731f6b38199ce8d7d0794c5e85.dl.dropboxusercontent.com/cd/0/inline/CfWhJRJZjR7JDyB-y8uZol1GztwgkwxY62Ga4F61-rcOvbLZjaXRhIXUck86i7vITQ8yyPVUT3-VkuVteQbdXaXicftb-t4PwWRvnTfJBSc8QVl0zA6ltqPgrEckejJfjuw/file?dl=1\n",
            "Resolving uc731f6b38199ce8d7d0794c5e85.dl.dropboxusercontent.com (uc731f6b38199ce8d7d0794c5e85.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6025:15::a27d:450f\n",
            "Connecting to uc731f6b38199ce8d7d0794c5e85.dl.dropboxusercontent.com (uc731f6b38199ce8d7d0794c5e85.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CfU_iohsW_6tQarbW6oSfPK8Dn_ABoQMctjqTw2_blL9KWP_1sE6-iDdibYhNoCpz2n4E8PPWxsNpsvU-eLkB8ZRiTUUofFBvpkO9lVBTdzoxxpgf4PYDE_2BUErPFW07LvRaMVJGV6X9WbhZFHdrJjt6q5ZbnOdNG-4cHFhwTEwXmE4233TOUeOOqqUE8Gd_-PWuweGrT26PYFhge9229p_IwGYQB2QQeo2wirEteBw7Dwljxj2J5zzH4cqU-NVhnwFIj-myK8ClAak1VvlgJSXxn9uzSxbcwv4Dk4mSI-F5cY6vyqPvaFWgg_TZh8mNrwGSUoKgbngwt9O0NUaLUyKjURDvkSYDzwDiZYnEK-Low/file?dl=1 [following]\n",
            "--2024-11-30 10:23:12--  https://uc731f6b38199ce8d7d0794c5e85.dl.dropboxusercontent.com/cd/0/inline2/CfU_iohsW_6tQarbW6oSfPK8Dn_ABoQMctjqTw2_blL9KWP_1sE6-iDdibYhNoCpz2n4E8PPWxsNpsvU-eLkB8ZRiTUUofFBvpkO9lVBTdzoxxpgf4PYDE_2BUErPFW07LvRaMVJGV6X9WbhZFHdrJjt6q5ZbnOdNG-4cHFhwTEwXmE4233TOUeOOqqUE8Gd_-PWuweGrT26PYFhge9229p_IwGYQB2QQeo2wirEteBw7Dwljxj2J5zzH4cqU-NVhnwFIj-myK8ClAak1VvlgJSXxn9uzSxbcwv4Dk4mSI-F5cY6vyqPvaFWgg_TZh8mNrwGSUoKgbngwt9O0NUaLUyKjURDvkSYDzwDiZYnEK-Low/file?dl=1\n",
            "Reusing existing connection to uc731f6b38199ce8d7d0794c5e85.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2333351997 (2.2G) [application/binary]\n",
            "Saving to: ‘id.tar.gz’\n",
            "\n",
            "id.tar.gz           100%[===================>]   2.17G  16.8MB/s    in 1m 52s  \n",
            "\n",
            "2024-11-30 10:25:04 (19.9 MB/s) - ‘id.tar.gz’ saved [2333351997/2333351997]\n",
            "\n",
            "FastText 'id.vec' berhasil dimuat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Memuat dataset dari file JSON\n",
        "with open('dataaa.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Mengubah dataset menjadi DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Penyesuaian nama kolom untuk konsistensi\n",
        "df.rename(columns={'utterance': 'utterances', 'response': 'responses'}, inplace=True)"
      ],
      "metadata": {
        "id": "e-Oe071h9alh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode intents\n",
        "label_encoder = LabelEncoder()\n",
        "df['intent_label'] = label_encoder.fit_transform(df['intent'])\n",
        "\n",
        "# Save intent mapping\n",
        "intent_mapping = dict(zip(df['intent_label'], df['intent']))\n",
        "\n",
        "# Menangani ketidakseimbangan data dengan oversampling\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X = df.index.values.reshape(-1, 1)\n",
        "y = df['intent_label']\n",
        "X_ros, y_ros = ros.fit_resample(X, y)\n",
        "\n",
        "# Membuat DataFrame baru dengan data yang telah dioversample\n",
        "df_balanced = df.loc[X_ros.flatten()].reset_index(drop=True)\n",
        "df_balanced['intent_label'] = y_ros\n",
        "df_balanced['intent'] = label_encoder.inverse_transform(df_balanced['intent_label'])"
      ],
      "metadata": {
        "id": "QZWTn0P29dly"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Memuat stopwords\n",
        "with open('stopword_list_tala.txt', 'r', encoding='utf-8') as f:\n",
        "    stop_words = f.read().splitlines()\n",
        "stop_words = set(stop_words)\n",
        "\n",
        "# Stemming dengan sastrawi\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# Daftar kata yang dikenal dari data\n",
        "all_text = ' '.join(df_balanced['utterances'])\n",
        "tokenizer_vocab = set(all_text.split())\n",
        "\n",
        "# Fungsi untuk memperbaiki kesalahan ejaan\n",
        "def correct_typo(text):\n",
        "    tokens = text.split()\n",
        "    corrected_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in tokenizer_vocab:\n",
        "            matches = process.extractOne(token, tokenizer_vocab)\n",
        "            if matches and matches[1] > 80:  # Threshold kecocokan\n",
        "                corrected_tokens.append(matches[0])\n",
        "            else:\n",
        "                corrected_tokens.append(token)\n",
        "        else:\n",
        "            corrected_tokens.append(token)\n",
        "    return ' '.join(corrected_tokens)\n",
        "\n",
        "# Fungsi preprocessing teks\n",
        "def preprocess_text(text):\n",
        "    text = clean_text(text)\n",
        "    text = correct_typo(text)\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Tambahkan pengecekan sebelum stemming\n",
        "    tokens_after_stemming = []\n",
        "    for token in tokens:\n",
        "        if token in ['hai', 'halo']:\n",
        "            tokens_after_stemming.append(token)\n",
        "        else:\n",
        "            stemmed_token = stemmer.stem(token)\n",
        "            tokens_after_stemming.append(stemmed_token)\n",
        "    text = ' '.join(tokens_after_stemming)\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df_balanced['utterances_clean'] = df_balanced['utterances'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "HYGOLdN-9gWP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare texts and labels\n",
        "texts = df_balanced['utterances_clean'].tolist()\n",
        "labels = df_balanced['intent_label'].tolist()\n",
        "\n",
        "# Split data untuk Klasifikasi Intents\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts,\n",
        "    labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "# Tokenisasi teks\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "\n",
        "# Padding sequences\n",
        "max_seq_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in val_sequences))\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_seq_length, padding='post')\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Convert labels to categorical\n",
        "num_classes = len(label_encoder.classes_)\n",
        "train_labels_cat = to_categorical(train_labels, num_classes=num_classes)\n",
        "val_labels_cat = to_categorical(val_labels, num_classes=num_classes)\n",
        "\n",
        "# Buat matriks embedding menggunakan FastText\n",
        "embedding_dim = fasttext_model.vector_size\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, idx in word_index.items():\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix[idx] = fasttext_model[word]\n",
        "    else:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))"
      ],
      "metadata": {
        "id": "-iesIHGK9ijt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter data dengan entities\n",
        "df_ner = df[df['entities'].map(lambda d: len(d)) > 0].reset_index(drop=True)\n",
        "df_ner['utterances_clean'] = df_ner['utterances'].apply(preprocess_text)\n",
        "\n",
        "def prepare_ner_data(df, tokenizer, max_seq_length):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for index, row in df.iterrows():\n",
        "        text = row['utterances_clean']\n",
        "        entities = row['entities']\n",
        "        tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "        label_seq = ['O'] * len(tokens)\n",
        "        for ent in entities:\n",
        "            ent_text = preprocess_text(ent['value'])\n",
        "            ent_tokens = tokenizer.texts_to_sequences([ent_text])[0]\n",
        "            ent_len = len(ent_tokens)\n",
        "            for i in range(len(tokens) - ent_len + 1):\n",
        "                if tokens[i:i+ent_len] == ent_tokens:\n",
        "                    label_seq[i] = 'B-' + ent['entity']\n",
        "                    for j in range(1, ent_len):\n",
        "                        label_seq[i+j] = 'I-' + ent['entity']\n",
        "                    break\n",
        "        texts.append(tokens)\n",
        "        labels.append(label_seq)\n",
        "    # Padding\n",
        "    texts_padded = pad_sequences(texts, maxlen=max_seq_length, padding='post')\n",
        "    # Padding labels\n",
        "    labels_padded = [label + ['O']*(max_seq_length - len(label)) for label in labels]\n",
        "    return texts_padded, labels_padded\n",
        "\n",
        "# Buat label encoder untuk NER\n",
        "all_labels = set()\n",
        "for label_list in df_ner['entities']:\n",
        "    for ent in label_list:\n",
        "        all_labels.add('B-' + ent['entity'])\n",
        "        all_labels.add('I-' + ent['entity'])\n",
        "all_labels.add('O')\n",
        "ner_label_encoder = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
        "ner_label_decoder = {idx: label for label, idx in ner_label_encoder.items()}\n",
        "\n",
        "# Prepare NER data\n",
        "texts_ner, labels_ner = prepare_ner_data(df_ner, tokenizer, max_seq_length)\n",
        "\n",
        "# Convert labels to numerical and categorical format\n",
        "def encode_ner_labels(labels, ner_label_encoder):\n",
        "    labels_encoded = []\n",
        "    for label_seq in labels:\n",
        "        label_ids = [ner_label_encoder[label] for label in label_seq]\n",
        "        labels_encoded.append(label_ids)\n",
        "    labels_encoded = np.array(labels_encoded)\n",
        "    labels_encoded = to_categorical(labels_encoded, num_classes=len(ner_label_encoder))\n",
        "    return labels_encoded\n",
        "\n",
        "labels_ner_encoded = encode_ner_labels(labels_ner, ner_label_encoder)\n",
        "\n",
        "# Split data untuk NER\n",
        "train_texts_ner, val_texts_ner, train_labels_ner, val_labels_ner = train_test_split(\n",
        "    texts_ner,\n",
        "    labels_ner_encoded,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        ")"
      ],
      "metadata": {
        "id": "gOaF2HSV9m84"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_intent_model_with_cnn(embedding_matrix, max_seq_length, num_classes, l2_reg=0.001):\n",
        "    inputs = Input(shape=(max_seq_length,))\n",
        "    embedding = tf.keras.layers.Embedding(\n",
        "        input_dim=embedding_matrix.shape[0],\n",
        "        output_dim=embedding_matrix.shape[1],\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=max_seq_length,\n",
        "        trainable=True\n",
        "    )(inputs)\n",
        "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedding)\n",
        "    global_pool = GlobalMaxPooling1D()(conv)\n",
        "    dense = Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(global_pool)\n",
        "    dropout = Dropout(0.5)(dense)\n",
        "    outputs = Dense(num_classes, activation='softmax')(dropout)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model_intent_cnn = build_intent_model_with_cnn(embedding_matrix, max_seq_length, num_classes, l2_reg=0.001)\n",
        "model_intent_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_intent_cnn.summary()\n",
        "\n",
        "# Early stopping\n",
        "callbacks_intent = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "# Melatih model\n",
        "history_intent_cnn = model_intent_cnn.fit(\n",
        "    train_padded,\n",
        "    train_labels_cat,\n",
        "    validation_data=(val_padded, val_labels_cat),\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    callbacks=callbacks_intent\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "id": "eDZXtsnV9peu",
        "outputId": "dd72decb-f4b5-4b04-cf04-e959854f6c58"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m300\u001b[0m)             │         \u001b[38;5;34m185,100\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m115,328\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_max_pooling1d                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m)                  │           \u001b[38;5;34m3,640\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">185,100</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">115,328</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_max_pooling1d                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,640</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m312,324\u001b[0m (1.19 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">312,324</span> (1.19 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m312,324\u001b[0m (1.19 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">312,324</span> (1.19 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.1468 - loss: 3.7291 - val_accuracy: 0.7290 - val_loss: 2.0415\n",
            "Epoch 2/20\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.5678 - loss: 1.9622 - val_accuracy: 0.8724 - val_loss: 0.8463\n",
            "Epoch 3/20\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7642 - loss: 1.0759 - val_accuracy: 0.9056 - val_loss: 0.5439\n",
            "Epoch 4/20\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.8487 - loss: 0.6847 - val_accuracy: 0.9108 - val_loss: 0.4271\n",
            "Epoch 5/20\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8926 - loss: 0.5295 - val_accuracy: 0.9388 - val_loss: 0.3694\n",
            "Epoch 6/20\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9226 - loss: 0.3876 - val_accuracy: 0.9441 - val_loss: 0.3428\n",
            "Epoch 7/20\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9366 - loss: 0.3486 - val_accuracy: 0.9406 - val_loss: 0.3421\n",
            "Epoch 8/20\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9409 - loss: 0.3180 - val_accuracy: 0.9388 - val_loss: 0.3426\n",
            "Epoch 9/20\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9429 - loss: 0.2872 - val_accuracy: 0.9388 - val_loss: 0.3572\n",
            "Epoch 10/20\n",
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.9565 - loss: 0.2667 - val_accuracy: 0.9353 - val_loss: 0.3611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NERHyperModel(HyperModel):\n",
        "    def __init__(self, embedding_matrix, max_seq_length, num_entities):\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.num_entities = num_entities\n",
        "\n",
        "    def build(self, hp):\n",
        "        l2_reg = hp.Choice('l2_reg', values=[1e-4, 1e-3, 1e-2])\n",
        "        dropout_rate = hp.Float('dropout_rate', 0.3, 0.7, step=0.1)\n",
        "        lstm_units = hp.Int('lstm_units', min_value=32, max_value=128, step=32)\n",
        "\n",
        "        inputs = Input(shape=(self.max_seq_length,))\n",
        "        embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=self.embedding_matrix.shape[0],\n",
        "            output_dim=self.embedding_matrix.shape[1],\n",
        "            weights=[self.embedding_matrix],\n",
        "            input_length=self.max_seq_length,\n",
        "            trainable=True\n",
        "        )(inputs)\n",
        "        lstm = Bidirectional(LSTM(lstm_units, kernel_regularizer=l2(l2_reg), return_sequences=True))(embedding)\n",
        "        dropout = Dropout(dropout_rate)(lstm)\n",
        "        outputs = TimeDistributed(Dense(self.num_entities, activation='softmax'))(dropout)\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "# Initialize HyperModel\n",
        "ner_hypermodel = NERHyperModel(embedding_matrix, max_seq_length, len(ner_label_encoder))\n",
        "\n",
        "# Initialize RandomSearch\n",
        "tuner_ner = RandomSearch(\n",
        "    ner_hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='ner_tuner_dir',\n",
        "    project_name='ner_tuning'\n",
        ")\n",
        "\n",
        "# Hyperparameter search for NER\n",
        "tuner_ner.search(\n",
        "    train_texts_ner,\n",
        "    train_labels_ner,\n",
        "    epochs=10,\n",
        "    validation_data=(val_texts_ner, val_labels_ner),\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Get the best model for NER\n",
        "best_model_ner = tuner_ner.get_best_models(num_models=1)[0]\n",
        "best_hp_ner = tuner_ner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best Hyperparameters for NER: {best_hp_ner.values}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvQnIddx9swk",
        "outputId": "cfc44e32-43b6-4e3c-a815-2693805d8ac8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 34s]\n",
            "val_accuracy: 0.916827917098999\n",
            "\n",
            "Best val_accuracy So Far: 0.95099937915802\n",
            "Total elapsed time: 00h 05m 21s\n",
            "Best Hyperparameters for NER: {'l2_reg': 0.0001, 'dropout_rate': 0.6000000000000001, 'lstm_units': 128}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 20 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi Model Intent Classification\n",
        "loss_intent, accuracy_intent = model_intent_cnn.evaluate(val_padded, val_labels_cat)\n",
        "print(f'Akurasi Model Klasifikasi Intent: {accuracy_intent * 100:.2f}%')\n",
        "\n",
        "# Evaluasi Model NER\n",
        "loss_ner, accuracy_ner = best_model_ner.evaluate(val_texts_ner, val_labels_ner)\n",
        "print(f'Akurasi Model NER: {accuracy_ner * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9WJDS1E93U4",
        "outputId": "37321f7b-28d8-4e6e-b391-35839ca356d6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9259 - loss: 0.3619\n",
            "Akurasi Model Klasifikasi Intent: 94.06%\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.9494 - loss: 0.2379\n",
            "Akurasi Model NER: 95.10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat direktori\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('encoders', exist_ok=True)\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Save intent model\n",
        "model_intent_cnn.save('models/model_intent.keras')\n",
        "\n",
        "# Save NER model\n",
        "best_model_ner.save('models/model_ner.keras')\n",
        "\n",
        "# Save tokenizer\n",
        "with open('encoders/tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Save label encoder\n",
        "with open('encoders/label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Save NER label encoder\n",
        "with open('encoders/ner_label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(ner_label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "Ig_Rk2zS9_w5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame for utterances and responses\n",
        "df_utterances = df_balanced[['utterances', 'responses', 'intent']].reset_index(drop=True)\n",
        "df_utterances['utterances_clean'] = df_utterances['utterances'].apply(preprocess_text)\n",
        "\n",
        "# Create TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(df_utterances['utterances_clean'])\n",
        "\n",
        "# Simpan vectorizer\n",
        "with open('data/vectorizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "o_UxveEW-Fem"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_intent(text):\n",
        "    text_clean = preprocess_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "    pred = model_intent_cnn.predict(padded_seq)\n",
        "    predicted_label = np.argmax(pred, axis=1)[0]\n",
        "    intent = label_encoder.inverse_transform([predicted_label])[0]\n",
        "    return intent\n",
        "\n",
        "def predict_entities(text):\n",
        "    text_clean = preprocess_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "    pred = best_model_ner.predict(padded_seq)\n",
        "    pred_labels = np.argmax(pred, axis=-1)[0]\n",
        "    tokens = tokenizer.sequences_to_texts(seq)[0].split()\n",
        "    entities = []\n",
        "    for idx, label_id in enumerate(pred_labels[:len(tokens)]):\n",
        "        label = ner_label_decoder[label_id]\n",
        "        if label != 'O':\n",
        "            entities.append({'entity': label.split('-')[1], 'value': tokens[idx]})\n",
        "    return entities"
      ],
      "metadata": {
        "id": "ZLBNJwp8-Hre"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping antara intent dan hewan terkait\n",
        "intent_animal_mapping = {\n",
        "    'medical_inquiry_dog': 'anjing',\n",
        "    'medical_inquiry_cat': 'kucing',\n",
        "    'symptom_analysis_dog': 'anjing',\n",
        "    'symptom_analysis_cat': 'kucing',\n",
        "    'disease_prevention_dog': 'anjing',\n",
        "    'disease_prevention_cat': 'kucing',\n",
        "    'dog_healthcare': 'anjing',\n",
        "    'cat_healthcare': 'kucing',\n",
        "    'animal_health_issue': ['anjing', 'kucing']\n",
        "}\n",
        "\n",
        "# Fungsi untuk menyesuaikan intent berdasarkan entitas\n",
        "def adjust_intent(intent, entities):\n",
        "    # Dapatkan hewan dari intent yang diprediksi\n",
        "    predicted_animal = intent_animal_mapping.get(intent, None)\n",
        "\n",
        "    # Ekstrak entitas 'animal' dari input pengguna\n",
        "    entity_animals = [ent['value'].lower() for ent in entities if ent['entity'] == 'animal']\n",
        "\n",
        "    if entity_animals:\n",
        "        user_animal = entity_animals[0]\n",
        "        if predicted_animal:\n",
        "            if isinstance(predicted_animal, list):\n",
        "                if user_animal not in predicted_animal:\n",
        "                    intent = None\n",
        "            else:\n",
        "                if predicted_animal != user_animal:\n",
        "                    for intent_name, animal in intent_animal_mapping.items():\n",
        "                        if animal == user_animal and intent_name != intent:\n",
        "                            intent = intent_name\n",
        "                            break\n",
        "                    else:\n",
        "                        intent = None\n",
        "    else:\n",
        "        # Jika tidak ada entitas 'animal', dan intent membutuhkan hewan tertentu, set intent ke None\n",
        "        if intent in intent_animal_mapping:\n",
        "            intent = None\n",
        "    return intent\n",
        "\n",
        "# Fungsi untuk mendapatkan respon berdasarkan intent yang disesuaikan\n",
        "def get_response(user_input, intent=None, entities=None):\n",
        "    user_input_clean = preprocess_text(user_input)\n",
        "    print(f\"Input yang Dipreproses: {user_input_clean}\")\n",
        "\n",
        "    if intent:\n",
        "        # Filter dataset berdasarkan intent yang disesuaikan\n",
        "        df_intent = df_utterances[df_utterances['intent'] == intent]\n",
        "        if df_intent.empty:\n",
        "            print(\"Intent tidak ditemukan dalam dataset.\")\n",
        "            return get_default_response()\n",
        "        else:\n",
        "            # Vectorize ulang utterances yang difilter\n",
        "            tfidf_matrix_intent = vectorizer.transform(df_intent['utterances_clean'])\n",
        "            user_tfidf = vectorizer.transform([user_input_clean])\n",
        "            similarities = cosine_similarity(user_tfidf, tfidf_matrix_intent)\n",
        "            most_similar_idx = np.argmax(similarities[0])\n",
        "            highest_similarity = similarities[0][most_similar_idx]\n",
        "            print(f\"Kemiripan Tertinggi: {highest_similarity}\")\n",
        "            if highest_similarity < 0.2:\n",
        "                print(\"Kemiripan di bawah threshold.\")\n",
        "                return get_default_response()\n",
        "            else:\n",
        "                # Ambil respon yang sesuai dengan utterance paling mirip\n",
        "                response = df_intent.iloc[most_similar_idx]['responses']\n",
        "                print(f\"Respon yang Dipilih: {response}\")\n",
        "                return response\n",
        "    else:\n",
        "        print(\"Intent tidak tersedia.\")\n",
        "        return get_default_response()"
      ],
      "metadata": {
        "id": "OtgGue60-gUK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_response(user_input):\n",
        "    # Prediksi intent dan entitas\n",
        "    intent = predict_intent(user_input)\n",
        "    entities = predict_entities(user_input)\n",
        "\n",
        "    # Sesuaikan intent berdasarkan entitas\n",
        "    adjusted_intent = adjust_intent(intent, entities)\n",
        "\n",
        "    print(f\"Intent yang Diprediksi: {intent}\")\n",
        "    print(f\"Entitas yang Diekstrak: {entities}\")\n",
        "    print(f\"Intent yang Disesuaikan: {adjusted_intent}\")\n",
        "\n",
        "    # Jika intent setelah disesuaikan adalah None, berikan respon default\n",
        "    if adjusted_intent is None:\n",
        "        response = get_default_response()\n",
        "    else:\n",
        "        # Dapatkan respon berdasarkan intent yang disesuaikan\n",
        "        response = get_response(user_input, adjusted_intent, entities)\n",
        "        # Jika tidak ada respon yang ditemukan, gunakan respon default\n",
        "        if not response:\n",
        "            response = get_default_response()\n",
        "    return response"
      ],
      "metadata": {
        "id": "ex81dOQt-ll6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk mendapatkan respon default\n",
        "def get_default_response():\n",
        "    default_responses = [\n",
        "        \"Maaf, saya belum bisa menjawab pertanyaan Anda.\",\n",
        "        \"Maaf, mohon diperjelas apa yang Anda maksud.\",\n",
        "        \"Maaf, saya hanya diprogram untuk menjawab pertanyaan mengenai kucing dan anjing.\",\n",
        "        \"Mohon maaf, saya tidak mengerti. Bisa dijelaskan lebih detail?\",\n",
        "        \"Saya belum memiliki informasi mengenai hal tersebut.\"\n",
        "    ]\n",
        "    return random.choice(default_responses)"
      ],
      "metadata": {
        "id": "SIpyyFeo-ohj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pengujian\n",
        "test_inputs = [\n",
        "    \"hai\",\n",
        "    \"halo\",\n",
        "    \"halo selamat pagi\",\n",
        "    \"kucing saya muntah dan diare\",\n",
        "    \"anjing saya matanya bengkak\",\n",
        "    \"apa itu toxoplasmosis\",\n",
        "    \"AI itu apa\",\n",
        "    \"anjing tetangga suka menggonggong\",\n",
        "    \"Saya melihat seekor anjing tua tanpa kalung di kompleks. Apa yang harus saya lakukan?\",\n",
        "    \"Saya menemukan kucing dengan mata tertutup kotoran di depan pasar. Apa yang harus saya lakukan?\",\n",
        "    \"Apakah saya bisa membawa anjing ke dalam kereta api jarak jauh? Jika ya, apa yang harus disiapkan?\",\n",
        "    \"Saya ingin membawa kucing saya dalam perjalanan ke luar kota menggunakan pesawat. Apa saja yang perlu dipersiapkan?\",\n",
        "    \"Kucing saya seperti atlet parkour, suka melompat ke rak dapur dan menjatuhkan barang. Bagaimana saya bisa menghentikannya?\",\n",
        "    \"Saya melihat kucing di gang kecil yang terus mondar-mandir dengan ekspresi kebingungan\",\n",
        "    \"Ada seekor anjing besar yang tampak sakit di depan kantor saya\",\n",
        "    \"pasar itu apa\",\n",
        "    \"apa yang dimaksud dengan analisa fundamental\",\n",
        "    \"terimakasih\",\n",
        "    \"makasih ya\"\n",
        "]\n",
        "\n",
        "for input_text in test_inputs:\n",
        "    print(f\"Anda: {input_text}\")\n",
        "    response = chatbot_response(input_text)\n",
        "    print(f\"Chatbot: {response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF1MIojJ-rb5",
        "outputId": "1c1d4737-04a7-40a8-9595-38c7aaf2c125"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anda: hai\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 563ms/step\n",
            "Intent yang Diprediksi: greeting\n",
            "Entitas yang Diekstrak: []\n",
            "Intent yang Disesuaikan: greeting\n",
            "Input yang Dipreproses: \n",
            "Kemiripan Tertinggi: 0.0\n",
            "Kemiripan di bawah threshold.\n",
            "Chatbot: Maaf, mohon diperjelas apa yang Anda maksud.\n",
            "\n",
            "Anda: halo\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Intent yang Diprediksi: greeting\n",
            "Entitas yang Diekstrak: []\n",
            "Intent yang Disesuaikan: greeting\n",
            "Input yang Dipreproses: \n",
            "Kemiripan Tertinggi: 0.0\n",
            "Kemiripan di bawah threshold.\n",
            "Chatbot: Maaf, saya hanya diprogram untuk menjawab pertanyaan mengenai kucing dan anjing.\n",
            "\n",
            "Anda: halo selamat pagi\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Intent yang Diprediksi: greeting\n",
            "Entitas yang Diekstrak: []\n",
            "Intent yang Disesuaikan: greeting\n",
            "Input yang Dipreproses: pagi\n",
            "Kemiripan Tertinggi: 1.0\n",
            "Respon yang Dipilih: Selamat pagi! Ada yang bisa saya bantu?\n",
            "Chatbot: Selamat pagi! Ada yang bisa saya bantu?\n",
            "\n",
            "Anda: kucing saya muntah dan diare\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "Intent yang Diprediksi: symptom_analysis_cat\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'kucing'}, {'entity': 'symptom', 'value': 'muntah'}, {'entity': 'symptom', 'value': 'diare'}]\n",
            "Intent yang Disesuaikan: symptom_analysis_cat\n",
            "Input yang Dipreproses: kucing muntah diare\n",
            "Kemiripan Tertinggi: 0.3519813548599023\n",
            "Respon yang Dipilih: Muntah dan dehidrasi bisa disebabkan oleh masalah pencernaan atau keracunan. Segera bawa kucing ke dokter hewan.\n",
            "Chatbot: Muntah dan dehidrasi bisa disebabkan oleh masalah pencernaan atau keracunan. Segera bawa kucing ke dokter hewan.\n",
            "\n",
            "Anda: anjing saya matanya bengkak\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Intent yang Diprediksi: symptom_analysis_dog\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'anjing'}, {'entity': 'symptom', 'value': 'mata'}, {'entity': 'symptom', 'value': 'bengkak'}]\n",
            "Intent yang Disesuaikan: symptom_analysis_dog\n",
            "Input yang Dipreproses: anjing mata bengkak\n",
            "Kemiripan Tertinggi: 0.41861040660560134\n",
            "Respon yang Dipilih: Kesulitan membuka mata dan mata berair bisa menjadi tanda iritasi atau infeksi. Bersihkan dengan kain lembut dan konsultasikan ke dokter hewan.\n",
            "Chatbot: Kesulitan membuka mata dan mata berair bisa menjadi tanda iritasi atau infeksi. Bersihkan dengan kain lembut dan konsultasikan ke dokter hewan.\n",
            "\n",
            "Anda: apa itu toxoplasmosis\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Intent yang Diprediksi: intro_chat\n",
            "Entitas yang Diekstrak: []\n",
            "Intent yang Disesuaikan: intro_chat\n",
            "Input yang Dipreproses: toxoplasmosis\n",
            "Kemiripan Tertinggi: 0.0\n",
            "Kemiripan di bawah threshold.\n",
            "Chatbot: Maaf, saya hanya diprogram untuk menjawab pertanyaan mengenai kucing dan anjing.\n",
            "\n",
            "Anda: AI itu apa\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Intent yang Diprediksi: greeting\n",
            "Entitas yang Diekstrak: []\n",
            "Intent yang Disesuaikan: greeting\n",
            "Input yang Dipreproses: \n",
            "Kemiripan Tertinggi: 0.0\n",
            "Kemiripan di bawah threshold.\n",
            "Chatbot: Maaf, saya belum bisa menjawab pertanyaan Anda.\n",
            "\n",
            "Anda: anjing tetangga suka menggonggong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Intent yang Diprediksi: report_animal_noise\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'anjing'}]\n",
            "Intent yang Disesuaikan: report_animal_noise\n",
            "Input yang Dipreproses: anjing tetangga suka gonggong\n",
            "Kemiripan Tertinggi: 0.49163917815096664\n",
            "Respon yang Dipilih: Coba bicarakan masalah ini dengan pemilik anjing. Jika tidak ada solusi, laporkan ke pengelola lingkungan.\n",
            "Chatbot: Coba bicarakan masalah ini dengan pemilik anjing. Jika tidak ada solusi, laporkan ke pengelola lingkungan.\n",
            "\n",
            "Anda: Saya melihat seekor anjing tua tanpa kalung di kompleks. Apa yang harus saya lakukan?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Intent yang Diprediksi: found_stray_dog_street\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'anjing'}, {'entity': 'condition', 'value': 'tua'}, {'entity': 'condition', 'value': 'kalung'}, {'entity': 'location', 'value': 'kompleks'}]\n",
            "Intent yang Disesuaikan: found_stray_dog_street\n",
            "Input yang Dipreproses: ekor anjing tua kalung kompleks laku\n",
            "Kemiripan Tertinggi: 1.0\n",
            "Respon yang Dipilih: Berikan air dan makanan, lalu coba cari informasi apakah ada pemilik yang kehilangan anjing.\n",
            "Chatbot: Berikan air dan makanan, lalu coba cari informasi apakah ada pemilik yang kehilangan anjing.\n",
            "\n",
            "Anda: Saya menemukan kucing dengan mata tertutup kotoran di depan pasar. Apa yang harus saya lakukan?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Intent yang Diprediksi: found_cat_eye_problem\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'kucing'}, {'entity': 'symptom', 'value': 'mata'}, {'entity': 'symptom', 'value': 'tutup'}, {'entity': 'symptom', 'value': 'kotor'}, {'entity': 'location', 'value': 'pasar'}]\n",
            "Intent yang Disesuaikan: found_cat_eye_problem\n",
            "Input yang Dipreproses: temu kucing mata tutup kotor pasar laku\n",
            "Kemiripan Tertinggi: 1.0\n",
            "Respon yang Dipilih: Bersihkan mata kucing dengan kapas lembut yang dibasahi air hangat, lalu bawa ke klinik hewan jika tidak membaik.\n",
            "Chatbot: Bersihkan mata kucing dengan kapas lembut yang dibasahi air hangat, lalu bawa ke klinik hewan jika tidak membaik.\n",
            "\n",
            "Anda: Apakah saya bisa membawa anjing ke dalam kereta api jarak jauh? Jika ya, apa yang harus disiapkan?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Intent yang Diprediksi: pet_transportation\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'anjing'}, {'entity': 'condition', 'value': 'kereta'}, {'entity': 'condition', 'value': 'api'}, {'entity': 'location', 'value': 'jarak'}]\n",
            "Intent yang Disesuaikan: pet_transportation\n",
            "Input yang Dipreproses: bawa anjing kereta api jarak jauh ya siap\n",
            "Kemiripan Tertinggi: 1.0000000000000002\n",
            "Respon yang Dipilih: Hubungi operator kereta untuk memastikan kebijakan mereka. Siapkan dokumen vaksinasi dan kandang sesuai aturan.\n",
            "Chatbot: Hubungi operator kereta untuk memastikan kebijakan mereka. Siapkan dokumen vaksinasi dan kandang sesuai aturan.\n",
            "\n",
            "Anda: Saya ingin membawa kucing saya dalam perjalanan ke luar kota menggunakan pesawat. Apa saja yang perlu dipersiapkan?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Intent yang Diprediksi: pet_transportation\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'kucing'}, {'entity': 'location', 'value': 'jalan'}]\n",
            "Intent yang Disesuaikan: pet_transportation\n",
            "Input yang Dipreproses: bawa kucing jalan kota pesawat siap\n",
            "Kemiripan Tertinggi: 1.0000000000000002\n",
            "Respon yang Dipilih: Pastikan kucing memiliki dokumen kesehatan, gunakan kandang yang sesuai, dan hubungi maskapai untuk informasi tambahan.\n",
            "Chatbot: Pastikan kucing memiliki dokumen kesehatan, gunakan kandang yang sesuai, dan hubungi maskapai untuk informasi tambahan.\n",
            "\n",
            "Anda: Kucing saya seperti atlet parkour, suka melompat ke rak dapur dan menjatuhkan barang. Bagaimana saya bisa menghentikannya?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "Intent yang Diprediksi: pet_behavior_issues\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'kucing'}, {'entity': 'behavior', 'value': 'rak'}, {'entity': 'behavior', 'value': 'dapur'}, {'entity': 'behavior', 'value': 'jatuh'}]\n",
            "Intent yang Disesuaikan: pet_behavior_issues\n",
            "Input yang Dipreproses: kucing atlet parkour suka lompat rak dapur jatuh barang henti\n",
            "Kemiripan Tertinggi: 1.0\n",
            "Respon yang Dipilih: Pastikan area dapur tidak menarik bagi kucing, seperti dengan menyimpan makanan di tempat tertutup. Berikan mainan atau tempat melompat alternatif.\n",
            "Chatbot: Pastikan area dapur tidak menarik bagi kucing, seperti dengan menyimpan makanan di tempat tertutup. Berikan mainan atau tempat melompat alternatif.\n",
            "\n",
            "Anda: Saya melihat kucing di gang kecil yang terus mondar-mandir dengan ekspresi kebingungan\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Intent yang Diprediksi: found_confused_cat\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'kucing'}]\n",
            "Intent yang Disesuaikan: found_confused_cat\n",
            "Input yang Dipreproses: kucing gang mondar-mandir ekspresi bingung\n",
            "Kemiripan Tertinggi: 0.9923566403798836\n",
            "Respon yang Dipilih: Dekati kucing tersebut secara perlahan. Jika memungkinkan, cek apakah kucing memiliki tanda pengenal.\n",
            "Chatbot: Dekati kucing tersebut secara perlahan. Jika memungkinkan, cek apakah kucing memiliki tanda pengenal.\n",
            "\n",
            "Anda: Ada seekor anjing besar yang tampak sakit di depan kantor saya\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Intent yang Diprediksi: found_animal\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'anjing'}, {'entity': 'location', 'value': 'kantor'}]\n",
            "Intent yang Disesuaikan: found_animal\n",
            "Input yang Dipreproses: ekor anjing sakit kantor\n",
            "Kemiripan Tertinggi: 0.6795756736983608\n",
            "Respon yang Dipilih: Dekati anjing dengan hati-hati, berikan air atau makanan ringan, dan hubungi dokter hewan atau komunitas penyelamat.\n",
            "Chatbot: Dekati anjing dengan hati-hati, berikan air atau makanan ringan, dan hubungi dokter hewan atau komunitas penyelamat.\n",
            "\n",
            "Anda: pasar itu apa\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Intent yang Diprediksi: intro_chat\n",
            "Entitas yang Diekstrak: [{'entity': 'location', 'value': 'pasar'}]\n",
            "Intent yang Disesuaikan: intro_chat\n",
            "Input yang Dipreproses: pasar\n",
            "Kemiripan Tertinggi: 0.0\n",
            "Kemiripan di bawah threshold.\n",
            "Chatbot: Maaf, mohon diperjelas apa yang Anda maksud.\n",
            "\n",
            "Anda: apa yang dimaksud dengan analisa fundamental\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Intent yang Diprediksi: intro_chat\n",
            "Entitas yang Diekstrak: []\n",
            "Intent yang Disesuaikan: intro_chat\n",
            "Input yang Dipreproses: analisa mental\n",
            "Kemiripan Tertinggi: 0.0\n",
            "Kemiripan di bawah threshold.\n",
            "Chatbot: Saya belum memiliki informasi mengenai hal tersebut.\n",
            "\n",
            "Anda: terimakasih\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "Intent yang Diprediksi: end_chat\n",
            "Entitas yang Diekstrak: []\n",
            "Intent yang Disesuaikan: end_chat\n",
            "Input yang Dipreproses: terimakasih\n",
            "Kemiripan Tertinggi: 1.0\n",
            "Respon yang Dipilih: Sama-sama! Jika ada pertanyaan lain, jangan ragu untuk bertanya kembali.\n",
            "Chatbot: Sama-sama! Jika ada pertanyaan lain, jangan ragu untuk bertanya kembali.\n",
            "\n",
            "Anda: makasih ya\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "Intent yang Diprediksi: end_chat\n",
            "Entitas yang Diekstrak: []\n",
            "Intent yang Disesuaikan: end_chat\n",
            "Input yang Dipreproses: terimakasih ya\n",
            "Kemiripan Tertinggi: 1.0\n",
            "Respon yang Dipilih: Sama-sama! Saya selalu siap membantu.\n",
            "Chatbot: Sama-sama! Saya selalu siap membantu.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir app\n",
        "!mv dataaa.json stopword_list_tala.txt data/\n",
        "!mv data app/\n",
        "!mv encoders app/\n",
        "!mv models app/"
      ],
      "metadata": {
        "id": "PeVFkVJj-vN6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r allinone.zip app/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEOh-KWIB4Du",
        "outputId": "953544f6-639f-4f53-ade8-a6e0451f495e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: app/ (stored 0%)\n",
            "  adding: app/models/ (stored 0%)\n",
            "  adding: app/models/model_intent.keras (deflated 8%)\n",
            "  adding: app/models/model_ner.keras (deflated 8%)\n",
            "  adding: app/data/ (stored 0%)\n",
            "  adding: app/data/vectorizer.pickle (deflated 50%)\n",
            "  adding: app/data/stopword_list_tala.txt (deflated 67%)\n",
            "  adding: app/data/dataaa.json (deflated 88%)\n",
            "  adding: app/encoders/ (stored 0%)\n",
            "  adding: app/encoders/tokenizer.pickle (deflated 44%)\n",
            "  adding: app/encoders/ner_label_encoder.pickle (deflated 44%)\n",
            "  adding: app/encoders/label_encoder.pickle (deflated 53%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Stemming 'hai': '{stemmer.stem('hai')}'\")\n",
        "print(f\"Stemming 'halo': '{stemmer.stem('halo')}'\")"
      ],
      "metadata": {
        "id": "bTTGeyE0CLRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b2ffb3-d283-45c1-dfa8-777988aaf400"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming 'hai': 'hai'\n",
            "Stemming 'halo': 'halo'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"hai\"\n",
        "user_input_clean = preprocess_text(user_input)\n",
        "print(f\"Setelah Preprocessing: '{user_input_clean}'\")"
      ],
      "metadata": {
        "id": "7UufmR7Topdc",
        "outputId": "bf41e6e4-b32b-4844-8e76-1936dc0d9576",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setelah Preprocessing: ''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hfhts-GKoyOy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}