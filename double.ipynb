{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSCLuOE/NspwP5Mlgq0skh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c1d62cebe6184d65b8559ee24480db09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Anda:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8bbb987ffad841af9bacd2113e9ba5b3",
            "placeholder": "Ketik pesan Anda...",
            "style": "IPY_MODEL_3431d1779bb1415a9546efe604ad3395",
            "value": ""
          }
        },
        "8bbb987ffad841af9bacd2113e9ba5b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3431d1779bb1415a9546efe604ad3395": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86ec88931a054d5586898b743e03bf79": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_425e27393d474c9d92a8beffdf9708f1",
            "msg_id": "",
            "outputs": []
          }
        },
        "425e27393d474c9d92a8beffdf9708f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanbayup/Machine-Learning/blob/main/double.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 1: Persiapan Lingkungan\n",
        "\n",
        "# Instalasi library yang diperlukan\n",
        "!pip install gensim\n",
        "!pip install tensorflow keras-tuner\n",
        "!pip install google-api-python-client\n",
        "!pip install seqeval\n",
        "!pip install imbalanced-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASTN06zWLyFn",
        "outputId": "8abcf881-540c-46ac-eecd-086f8fd0a851"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.151.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.25.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.8.30)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=f9ceb561baa681dec9a83d60b46f04522301acb81d4cd8048e0b3f3047f245c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import gensim\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from seqeval.metrics import classification_report as seq_classification_report\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from kerastuner import RandomSearch\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import pickle\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Langkah 2: Mount Google Drive (Opsional)\n",
        "# Jika Anda ingin menyimpan model dan data di Google Drive, aktifkan baris berikut:\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Langkah 3: Download dan Persiapan FastText Embeddings\n",
        "\n",
        "# URL FastText Bahasa Indonesia\n",
        "fasttext_url = 'https://www.dropbox.com/scl/fi/zjvzebgdklqosxpylu93b/id.tar.gz?rlkey=k8n322p8xteuog6lf3jdg2a6t&st=8wwkv788&dl=1'\n",
        "\n",
        "# Download FastText model\n",
        "!wget -O id.tar.gz \"{fasttext_url}\"\n",
        "!tar -xzf id.tar.gz\n",
        "\n",
        "# Asumsi setelah ekstraksi, Anda memiliki 'id.vec' dan 'id.bin'\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Muat model FastText menggunakan 'id.vec'\n",
        "try:\n",
        "    fasttext_model = KeyedVectors.load_word2vec_format('id.vec', binary=False)\n",
        "    print(\"Model FastText 'id.vec' berhasil dimuat.\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal memuat 'id.vec': {e}\")\n",
        "    # Jika gagal, coba muat 'id.bin'\n",
        "    try:\n",
        "        fasttext_model = KeyedVectors.load_facebook_vectors('id.bin')\n",
        "        print(\"Model FastText 'id.bin' berhasil dimuat.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Gagal memuat 'id.bin': {e}\")\n",
        "        raise ValueError(\"Gagal memuat model FastText. Pastikan file 'id.vec' atau 'id.bin' dalam format yang benar.\")\n",
        "\n",
        "# Langkah 4: Preprocessing Data\n",
        "\n",
        "# Memuat dataset dari file JSON\n",
        "with open('dataaa.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Mengubah dataset menjadi DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Menentukan jumlah sampel maksimum per intent\n",
        "max_samples = 50\n",
        "\n",
        "df_list = []\n",
        "for intent in df['intent'].unique():\n",
        "    df_intent = df[df['intent'] == intent]\n",
        "    if len(df_intent) > max_samples:\n",
        "        df_intent = resample(df_intent, replace=False, n_samples=max_samples, random_state=42)\n",
        "    df_list.append(df_intent)\n",
        "\n",
        "df_balanced = pd.concat(df_list).reset_index(drop=True)\n",
        "\n",
        "# Encode intents\n",
        "label_encoder = LabelEncoder()\n",
        "df_balanced['intent_label'] = label_encoder.fit_transform(df_balanced['intent'])\n",
        "\n",
        "# Simpan mapping label untuk penggunaan nanti\n",
        "intent_mapping = dict(zip(df_balanced['intent_label'], df_balanced['intent']))\n",
        "\n",
        "# Mengatasi kelas imbalanced dengan oversampling\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "# Sertakan 'responses' dalam X untuk menjaga pasangan utterances-responses\n",
        "X_ros, y_ros = ros.fit_resample(df_balanced[['utterances', 'responses']], df_balanced['intent_label'])\n",
        "\n",
        "df_balanced = pd.DataFrame({\n",
        "    'utterances': X_ros['utterances'],\n",
        "    'responses': X_ros['responses'],\n",
        "    'intent_label': y_ros\n",
        "})\n",
        "\n",
        "# Memfilter data yang memiliki entitas\n",
        "df_ner = df[df['entities'].map(lambda d: len(d)) > 0].reset_index(drop=True)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Pembersihan teks\n",
        "df_balanced['utterances_clean'] = df_balanced['utterances'].apply(clean_text)\n",
        "df_ner['utterances_clean'] = df_ner['utterances'].apply(clean_text)\n",
        "\n",
        "texts = df_balanced['utterances_clean'].tolist()\n",
        "labels = df_balanced['intent_label'].tolist()\n",
        "\n",
        "# Split data untuk Klasifikasi Intent\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts,\n",
        "    labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "# Tokenisasi\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "# Mengonversi teks ke sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "\n",
        "# Padding sequences\n",
        "max_seq_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in val_sequences))\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_seq_length, padding='post')\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Mengonversi labels ke categorical\n",
        "num_classes = len(label_encoder.classes_)\n",
        "train_labels_cat = to_categorical(train_labels, num_classes=num_classes)\n",
        "val_labels_cat = to_categorical(val_labels, num_classes=num_classes)\n",
        "\n",
        "# Membuat embedding matrix menggunakan FastText\n",
        "embedding_dim = fasttext_model.vector_size  # 300\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, idx in word_index.items():\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix[idx] = fasttext_model[word]\n",
        "    else:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "\n",
        "# Langkah 5: Persiapan Data untuk NER\n",
        "\n",
        "def prepare_ner_data(df, tokenizer, max_seq_length):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for index, row in df.iterrows():\n",
        "        text = row['utterances_clean']\n",
        "        entities = row['entities']\n",
        "        tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "        label_seq = ['O'] * len(tokens)\n",
        "        for ent in entities:\n",
        "            ent_text = clean_text(ent['value'])\n",
        "            ent_tokens = tokenizer.texts_to_sequences([ent_text])[0]\n",
        "            ent_len = len(ent_tokens)\n",
        "            for i in range(len(tokens) - ent_len + 1):\n",
        "                if tokens[i:i+ent_len] == ent_tokens:\n",
        "                    label_seq[i] = 'B-' + ent['entity']\n",
        "                    for j in range(1, ent_len):\n",
        "                        label_seq[i+j] = 'I-' + ent['entity']\n",
        "                    break\n",
        "        texts.append(tokens)\n",
        "        labels.append(label_seq)\n",
        "    # Padding\n",
        "    texts_padded = pad_sequences(texts, maxlen=max_seq_length, padding='post')\n",
        "    # Padding labels\n",
        "    labels_padded = [label + ['O']*(max_seq_length - len(label)) for label in labels]\n",
        "    return texts_padded, labels_padded\n",
        "\n",
        "# Membuat label encoder untuk NER\n",
        "all_labels = set()\n",
        "for label_list in df_ner['entities']:\n",
        "    for ent in label_list:\n",
        "        all_labels.add('B-' + ent['entity'])\n",
        "        all_labels.add('I-' + ent['entity'])\n",
        "all_labels.add('O')\n",
        "ner_label_encoder = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
        "ner_label_decoder = {idx: label for label, idx in ner_label_encoder.items()}\n",
        "\n",
        "# Siapkan data NER\n",
        "texts_ner, labels_ner = prepare_ner_data(df_ner, tokenizer, max_seq_length)\n",
        "\n",
        "# Mengonversi labels ke format numerik dan categorical\n",
        "def encode_ner_labels(labels, ner_label_encoder):\n",
        "    labels_encoded = []\n",
        "    for label_seq in labels:\n",
        "        label_ids = [ner_label_encoder[label] for label in label_seq]\n",
        "        labels_encoded.append(label_ids)\n",
        "    labels_encoded = np.array(labels_encoded)\n",
        "    labels_encoded = to_categorical(labels_encoded, num_classes=len(ner_label_encoder))\n",
        "    return labels_encoded\n",
        "\n",
        "labels_ner_encoded = encode_ner_labels(labels_ner, ner_label_encoder)\n",
        "\n",
        "# Split data untuk NER\n",
        "train_texts_ner, val_texts_ner, train_labels_ner, val_labels_ner = train_test_split(\n",
        "    texts_ner,\n",
        "    labels_ner_encoded,\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Langkah 6: Definisi dan Kompilasi Model\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, Bidirectional, LSTM, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Definisikan Fungsi untuk Membangun Model Klasifikasi Intent\n",
        "def build_intent_model(embedding_matrix, max_seq_length, num_classes, l2_reg=0.001):\n",
        "    inputs = Input(shape=(max_seq_length,))\n",
        "    embedding = tf.keras.layers.Embedding(\n",
        "        input_dim=embedding_matrix.shape[0],\n",
        "        output_dim=embedding_matrix.shape[1],\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=max_seq_length,\n",
        "        trainable=False\n",
        "    )(inputs)\n",
        "    lstm = Bidirectional(LSTM(64, kernel_regularizer=l2(l2_reg), return_sequences=False))(embedding)\n",
        "    dense = Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(lstm)\n",
        "    dropout = Dropout(0.5)(dense)\n",
        "    outputs = Dense(num_classes, activation='softmax')(dropout)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Definisikan Fungsi untuk Membangun Model NER\n",
        "def build_ner_model(embedding_matrix, max_seq_length, num_entities, l2_reg=0.001):\n",
        "    inputs = Input(shape=(max_seq_length,))\n",
        "    embedding = tf.keras.layers.Embedding(\n",
        "        input_dim=embedding_matrix.shape[0],\n",
        "        output_dim=embedding_matrix.shape[1],\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=max_seq_length,\n",
        "        trainable=False\n",
        "    )(inputs)\n",
        "    lstm = Bidirectional(LSTM(64, kernel_regularizer=l2(l2_reg), return_sequences=True))(embedding)\n",
        "    dropout = Dropout(0.5)(lstm)\n",
        "    outputs = TimeDistributed(Dense(num_entities, activation='softmax', kernel_regularizer=l2(l2_reg)))(dropout)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Membangun Model Klasifikasi Intent\n",
        "model_intent = build_intent_model(embedding_matrix, max_seq_length, num_classes, l2_reg=0.001)\n",
        "model_intent.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_intent.summary()\n",
        "\n",
        "# Membangun Model NER\n",
        "model_ner = build_ner_model(embedding_matrix, max_seq_length, len(ner_label_encoder), l2_reg=0.001)\n",
        "model_ner.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_ner.summary()\n",
        "\n",
        "# Langkah 7: Pelatihan Model dengan Hyperparameter Optimization\n",
        "\n",
        "from kerastuner import HyperModel\n",
        "\n",
        "# Definisikan HyperModel untuk Intent Classification\n",
        "class IntentHyperModel(HyperModel):\n",
        "    def __init__(self, embedding_matrix, max_seq_length, num_classes):\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def build(self, hp):\n",
        "        l2_reg = hp.Choice('l2_reg', values=[1e-4, 1e-3, 1e-2])\n",
        "        dropout_rate = hp.Float('dropout_rate', 0.3, 0.7, step=0.1)\n",
        "        lstm_units = hp.Int('lstm_units', min_value=32, max_value=128, step=32)\n",
        "        dense_units = hp.Int('dense_units', min_value=32, max_value=128, step=32)\n",
        "\n",
        "        inputs = Input(shape=(self.max_seq_length,))\n",
        "        embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=self.embedding_matrix.shape[0],\n",
        "            output_dim=self.embedding_matrix.shape[1],\n",
        "            weights=[self.embedding_matrix],\n",
        "            input_length=self.max_seq_length,\n",
        "            trainable=False\n",
        "        )(inputs)\n",
        "        lstm = Bidirectional(LSTM(lstm_units, kernel_regularizer=l2(l2_reg), return_sequences=False))(embedding)\n",
        "        dense = Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg))(lstm)\n",
        "        dropout = Dropout(dropout_rate)(dense)\n",
        "        outputs = Dense(self.num_classes, activation='softmax')(dropout)\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "# Inisialisasi HyperModel\n",
        "intent_hypermodel = IntentHyperModel(embedding_matrix, max_seq_length, num_classes)\n",
        "\n",
        "# Inisialisasi RandomSearch\n",
        "tuner_intent = RandomSearch(\n",
        "    intent_hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    directory='intent_tuner_dir',\n",
        "    project_name='intent_classification'\n",
        ")\n",
        "\n",
        "# Menampilkan ringkasan tuner\n",
        "tuner_intent.search_space_summary()\n",
        "\n",
        "# Pencarian Hyperparameter untuk Intent Classification\n",
        "tuner_intent.search(\n",
        "    train_padded,\n",
        "    train_labels_cat,\n",
        "    epochs=10,\n",
        "    validation_data=(val_padded, val_labels_cat),\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Mendapatkan model terbaik untuk Intent Classification\n",
        "best_model_intent = tuner_intent.get_best_models(num_models=1)[0]\n",
        "best_hp_intent = tuner_intent.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best Hyperparameters for Intent Classification: {best_hp_intent.values}\")\n",
        "\n",
        "# Mempersiapkan Callback untuk NER\n",
        "callbacks_ner = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    ModelCheckpoint(\n",
        "        filepath='best_model_ner.keras',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False\n",
        "    )\n",
        "]\n",
        "\n",
        "# Melatih Model NER\n",
        "history_ner = model_ner.fit(\n",
        "    train_texts_ner,\n",
        "    train_labels_ner,\n",
        "    validation_data=(val_texts_ner, val_labels_ner),\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    callbacks=callbacks_ner\n",
        ")\n",
        "\n",
        "# Langkah 8: Evaluasi Model\n",
        "\n",
        "# Evaluasi Klasifikasi Intent\n",
        "loss_intent, accuracy_intent = best_model_intent.evaluate(val_padded, val_labels_cat)\n",
        "print(f'Akurasi Model Klasifikasi Intent: {accuracy_intent * 100:.2f}%')\n",
        "\n",
        "# Prediksi pada data validasi Intent\n",
        "val_preds_intent = best_model_intent.predict(val_padded)\n",
        "val_preds_intent = np.argmax(val_preds_intent, axis=1)\n",
        "val_true_intent = np.argmax(val_labels_cat, axis=1)\n",
        "\n",
        "# Classification report untuk Intent\n",
        "print(\"Classification Report untuk Intent:\")\n",
        "print(classification_report(val_true_intent, val_preds_intent, target_names=label_encoder.classes_))\n",
        "\n",
        "# Evaluasi NER\n",
        "loss_ner, accuracy_ner = model_ner.evaluate(val_texts_ner, val_labels_ner)\n",
        "print(f'Akurasi Model NER: {accuracy_ner * 100:.2f}%')\n",
        "\n",
        "# Prediksi pada data validasi NER\n",
        "val_preds_ner = model_ner.predict(val_texts_ner)\n",
        "val_preds_ner = np.argmax(val_preds_ner, axis=-1)\n",
        "val_true_ner = np.argmax(val_labels_ner, axis=-1)\n",
        "\n",
        "# Konversi label ke format aslinya\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for i in range(len(val_preds_ner)):\n",
        "    true_label = []\n",
        "    pred_label = []\n",
        "    for j in range(len(val_preds_ner[i])):\n",
        "        true_l = ner_label_decoder[val_true_ner[i][j]]\n",
        "        pred_l = ner_label_decoder[val_preds_ner[i][j]]\n",
        "        if true_l != 'O':\n",
        "            true_label.append(true_l)\n",
        "            pred_label.append(pred_l)\n",
        "    true_labels.append(true_label)\n",
        "    pred_labels.append(pred_label)\n",
        "\n",
        "# Classification report untuk NER\n",
        "print(\"Classification Report untuk NER:\")\n",
        "print(seq_classification_report(true_labels, pred_labels))\n",
        "\n",
        "# Langkah 9: Penyimpanan dan Pemuatan Model\n",
        "\n",
        "# Simpan model\n",
        "best_model_intent.save('model_intent.keras')\n",
        "model_ner.save('model_ner.keras')\n",
        "\n",
        "# Simpan tokenizer dan label encoders\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('ner_label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(ner_label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Pemuatan model dan tokenizer (Jika diperlukan)\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_intent = load_model('model_intent.keras')\n",
        "model_ner = load_model('model_ner.keras')\n",
        "\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "with open('label_encoder.pickle', 'rb') as handle:\n",
        "    label_encoder = pickle.load(handle)\n",
        "with open('ner_label_encoder.pickle', 'rb') as handle:\n",
        "    ner_label_encoder = pickle.load(handle)\n",
        "ner_label_decoder = {idx: label for label, idx in ner_label_encoder.items()}\n",
        "\n",
        "# Langkah 10: Implementasi Chatbot\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Membuat DataFrame utterances dan responses\n",
        "df_utterances = df_balanced[['utterances', 'responses']].reset_index(drop=True)\n",
        "df_utterances['utterances_clean'] = df_utterances['utterances'].apply(clean_text)\n",
        "\n",
        "# Menghitung TF-IDF matrix\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(df_utterances['utterances_clean'])\n",
        "\n",
        "def predict_intent(text):\n",
        "    text_clean = clean_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "    pred = model_intent.predict(padded_seq)\n",
        "    predicted_label = np.argmax(pred, axis=1)[0]\n",
        "    intent = label_encoder.inverse_transform([predicted_label])[0]\n",
        "    return intent\n",
        "\n",
        "def predict_entities(text):\n",
        "    text_clean = clean_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "    pred = model_ner.predict(padded_seq)\n",
        "    pred_labels = np.argmax(pred, axis=-1)[0]\n",
        "    tokens = tokenizer.sequences_to_texts(seq)[0].split()\n",
        "    entities = []\n",
        "    for idx, label_id in enumerate(pred_labels[:len(tokens)]):\n",
        "        label = ner_label_decoder[label_id]\n",
        "        if label != 'O':\n",
        "            entities.append({'entity': label.split('-')[1], 'value': tokens[idx]})\n",
        "    return entities\n",
        "\n",
        "def get_response(user_input):\n",
        "    # Preprocess input\n",
        "    user_input_clean = clean_text(user_input)\n",
        "    user_tfidf = vectorizer.transform([user_input_clean])\n",
        "\n",
        "    # Hitung cosine similarity\n",
        "    similarities = cosine_similarity(user_tfidf, tfidf_matrix)\n",
        "\n",
        "    # Dapatkan indeks dengan similarity tertinggi\n",
        "    most_similar_idx = np.argmax(similarities[0])\n",
        "\n",
        "    # Jika similarity rendah, berikan respon default\n",
        "    if similarities[0][most_similar_idx] < 0.2:\n",
        "        return \"Maaf, saya tidak memahami pertanyaan Anda.\"\n",
        "\n",
        "    # Ambil respon yang sesuai\n",
        "    response = df_utterances.iloc[most_similar_idx]['responses']\n",
        "\n",
        "    return response\n",
        "\n",
        "def chatbot_response(user_input):\n",
        "    # Prediksi intent dan entitas\n",
        "    intent = predict_intent(user_input)\n",
        "    entities = predict_entities(user_input)\n",
        "\n",
        "    # Respon berbasis TF-IDF\n",
        "    response = get_response(user_input)\n",
        "\n",
        "    # Modifikasi respon berdasarkan entitas jika diperlukan\n",
        "    # Anda dapat menambahkan logika tambahan di sini\n",
        "\n",
        "    # Jika intent adalah 'general_conversation', hanya berikan respon TF-IDF\n",
        "    if intent == 'general_conversation':\n",
        "        return response\n",
        "    else:\n",
        "        # Tambahkan informasi tambahan atau respon spesifik berdasarkan intent\n",
        "        return response\n",
        "\n",
        "# Membuat widget input dan output\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "input_box = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Ketik pesan Anda...',\n",
        "    description='Anda:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def on_submit(sender):\n",
        "    user_input = input_box.value\n",
        "    input_box.value = ''\n",
        "    response = chatbot_response(user_input)\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        print(f\"Anda: {user_input}\")\n",
        "        print(f\"Chatbot: {response}\\n\")\n",
        "\n",
        "input_box.on_submit(on_submit)\n",
        "\n",
        "display(input_box, output_area)\n",
        "\n",
        "# Langkah 11: Contoh Penggunaan\n",
        "\n",
        "# Contoh kalimat\n",
        "test_sentence = \"Kucing saya sering muntah dan tidak mau makan.\"\n",
        "\n",
        "# Prediksi intent\n",
        "predicted_intent = predict_intent(test_sentence)\n",
        "print(f\"Intent yang diprediksi: {predicted_intent}\")\n",
        "\n",
        "# Prediksi entitas\n",
        "predicted_entities = predict_entities(test_sentence)\n",
        "print(\"Entitas yang ditemukan:\")\n",
        "for entity in predicted_entities:\n",
        "    print(f\"- {entity['entity']}: {entity['value']}\")\n",
        "\n",
        "# Mendapatkan respon\n",
        "response = chatbot_response(test_sentence)\n",
        "print(f\"Chatbot: {response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c1d62cebe6184d65b8559ee24480db09",
            "8bbb987ffad841af9bacd2113e9ba5b3",
            "3431d1779bb1415a9546efe604ad3395",
            "86ec88931a054d5586898b743e03bf79",
            "425e27393d474c9d92a8beffdf9708f1"
          ]
        },
        "id": "x35Id9ObLrVp",
        "outputId": "2d4e333b-f0ad-4bd4-805c-5a16cc41295a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 13s]\n",
            "val_accuracy: 0.921875\n",
            "\n",
            "Best val_accuracy So Far: 0.940625011920929\n",
            "Total elapsed time: 00h 02m 04s\n",
            "Best Hyperparameters for Intent Classification: {'l2_reg': 0.0001, 'dropout_rate': 0.3, 'lstm_units': 64, 'dense_units': 64}\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 22 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.5150 - loss: 2.8995 - val_accuracy: 0.8268 - val_loss: 1.3909\n",
            "Epoch 2/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7861 - loss: 1.4442 - val_accuracy: 0.8489 - val_loss: 1.0406\n",
            "Epoch 3/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8254 - loss: 1.0626 - val_accuracy: 0.8591 - val_loss: 0.8647\n",
            "Epoch 4/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8403 - loss: 0.8953 - val_accuracy: 0.8642 - val_loss: 0.7479\n",
            "Epoch 5/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8451 - loss: 0.7790 - val_accuracy: 0.8642 - val_loss: 0.6686\n",
            "Epoch 6/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8606 - loss: 0.6623 - val_accuracy: 0.8727 - val_loss: 0.6160\n",
            "Epoch 7/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8661 - loss: 0.6096 - val_accuracy: 0.8676 - val_loss: 0.5814\n",
            "Epoch 8/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8721 - loss: 0.5672 - val_accuracy: 0.8761 - val_loss: 0.5505\n",
            "Epoch 9/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8851 - loss: 0.5108 - val_accuracy: 0.8744 - val_loss: 0.5248\n",
            "Epoch 10/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8887 - loss: 0.4865 - val_accuracy: 0.8744 - val_loss: 0.5016\n",
            "Epoch 11/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8962 - loss: 0.4574 - val_accuracy: 0.8761 - val_loss: 0.4870\n",
            "Epoch 12/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9069 - loss: 0.4281 - val_accuracy: 0.8778 - val_loss: 0.4825\n",
            "Epoch 13/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9114 - loss: 0.4056 - val_accuracy: 0.8896 - val_loss: 0.4743\n",
            "Epoch 14/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9137 - loss: 0.3934 - val_accuracy: 0.8761 - val_loss: 0.4611\n",
            "Epoch 15/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9239 - loss: 0.3575 - val_accuracy: 0.8812 - val_loss: 0.4625\n",
            "Epoch 16/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9321 - loss: 0.3355 - val_accuracy: 0.8829 - val_loss: 0.4662\n",
            "Epoch 17/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9316 - loss: 0.3258 - val_accuracy: 0.8981 - val_loss: 0.4422\n",
            "Epoch 18/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9286 - loss: 0.3298 - val_accuracy: 0.8930 - val_loss: 0.4472\n",
            "Epoch 19/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9349 - loss: 0.3139 - val_accuracy: 0.8964 - val_loss: 0.4402\n",
            "Epoch 20/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9325 - loss: 0.3173 - val_accuracy: 0.8829 - val_loss: 0.4580\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9615 - loss: 0.2010  \n",
            "Akurasi Model Klasifikasi Intent: 95.63%\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step  \n",
            "Classification Report untuk Intent:\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "         cat_healthcare       1.00      1.00      1.00        10\n",
            " disease_prevention_cat       1.00      1.00      1.00        10\n",
            " disease_prevention_dog       1.00      1.00      1.00        10\n",
            "         dog_healthcare       1.00      1.00      1.00        10\n",
            "               end_chat       0.91      1.00      0.95        10\n",
            "               filsafat       0.90      0.90      0.90        10\n",
            "   general_conversation       0.69      0.90      0.78        10\n",
            "             intro_chat       0.90      0.90      0.90        10\n",
            "    medical_inquiry_cat       1.00      1.00      1.00        10\n",
            "    medical_inquiry_dog       1.00      1.00      1.00        10\n",
            "    pet_food_suggestion       1.00      1.00      1.00        10\n",
            "pet_health_status_check       1.00      1.00      1.00        10\n",
            "  pet_vaccination_check       1.00      1.00      1.00        10\n",
            "   science_conversation       1.00      0.60      0.75        10\n",
            "   symptom_analysis_cat       1.00      1.00      1.00        10\n",
            "   symptom_analysis_dog       1.00      1.00      1.00        10\n",
            "\n",
            "               accuracy                           0.96       160\n",
            "              macro avg       0.96      0.96      0.96       160\n",
            "           weighted avg       0.96      0.96      0.96       160\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8964 - loss: 0.4402\n",
            "Akurasi Model NER: 89.64%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step\n",
            "Classification Report untuk NER:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      animal       0.96      0.96      0.96        24\n",
            "   condition       0.00      0.00      0.00         2\n",
            "     disease       1.00      0.71      0.83         7\n",
            "    duration       0.00      0.00      0.00         1\n",
            "        food       0.00      0.00      0.00         2\n",
            "     symptom       0.50      0.35      0.41        20\n",
            "\n",
            "   micro avg       0.80      0.62      0.70        56\n",
            "   macro avg       0.41      0.34      0.37        56\n",
            "weighted avg       0.71      0.62      0.66        56\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 22 variables whereas the saved optimizer has 2 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Anda:', placeholder='Ketik pesan Anda...')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1d62cebe6184d65b8559ee24480db09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86ec88931a054d5586898b743e03bf79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
            "Intent yang diprediksi: pet_food_suggestion\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step\n",
            "Entitas yang ditemukan:\n",
            "- animal: kucing\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Chatbot: Berikan makanan ringan seperti nasi dan ikan rebus. Jika gejala tidak membaik dalam 24 jam, segera konsultasikan ke dokter hewan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-sHcani3MXYD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}