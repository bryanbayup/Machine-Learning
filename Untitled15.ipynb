{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyORMLK8QiBGRFbvz9UYN0OV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanbayup/Machine-Learning/blob/main/Untitled15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIdxHxQZPsZ-",
        "outputId": "437c944b-4013-46f6-c8f1-614f3218bcf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.13.0 in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.67.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.12.1)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.0) (0.45.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.13.0\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "DACFXj5ZP6pl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memuat dataset dari file JSON\n",
        "with open('dataaa.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "HRACWJy1QZUJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utterances = []\n",
        "intents = []\n",
        "entities_list = []\n",
        "\n",
        "for item in data:\n",
        "    utterances.append(item['utterances'])\n",
        "    intents.append(item['intent'])\n",
        "    entities_list.append(item['entities'])"
      ],
      "metadata": {
        "id": "6Nrwq3FOQd3q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "cleaned_utterances = [clean_text(utt) for utt in utterances]"
      ],
      "metadata": {
        "id": "JYIu9ZMlQgQK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(cleaned_utterances)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1"
      ],
      "metadata": {
        "id": "CLeCynrzQi0l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(cleaned_utterances)\n",
        "max_seq_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')"
      ],
      "metadata": {
        "id": "kczyU8AWQlGo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "intents_encoded = label_encoder.fit_transform(intents)\n",
        "num_intents = len(label_encoder.classes_)\n",
        "intents_one_hot = to_categorical(intents_encoded, num_classes=num_intents)"
      ],
      "metadata": {
        "id": "h1i1-_lsQn4l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat daftar semua entitas unik\n",
        "unique_entities = set()\n",
        "for entities in entities_list:\n",
        "    for ent in entities:\n",
        "        unique_entities.add(ent['entity'])\n",
        "\n",
        "entity_to_id = {entity: idx + 1 for idx, entity in enumerate(unique_entities)}  # Mulai dari 1\n",
        "entity_to_id['O'] = 0  # Label untuk 'Outside'\n",
        "num_entities = len(entity_to_id)\n",
        "\n",
        "# Membuat label NER\n",
        "ner_labels = []\n",
        "\n",
        "for i, entities in enumerate(entities_list):\n",
        "    seq_len = len(padded_sequences[i])\n",
        "    labels = ['O'] * seq_len\n",
        "    for ent in entities:\n",
        "        start = ent['start']\n",
        "        end = ent['end']\n",
        "        value = ent['value']\n",
        "        entity = ent['entity']\n",
        "        # Temukan indeks token yang sesuai\n",
        "        token_seq = sequences[i]\n",
        "        tokens = tokenizer.texts_to_sequences([clean_text(value)])[0]\n",
        "        for idx in range(len(token_seq)):\n",
        "            if token_seq[idx:idx+len(tokens)] == tokens:\n",
        "                for j in range(len(tokens)):\n",
        "                    labels[idx + j] = entity\n",
        "                break\n",
        "    # Konversi label ke ID\n",
        "    labels_id = [entity_to_id[label] for label in labels]\n",
        "    ner_labels.append(labels_id)\n",
        "\n",
        "ner_labels = pad_sequences(ner_labels, maxlen=max_seq_length, padding='post')\n",
        "ner_labels = [to_categorical(label_seq, num_classes=num_entities) for label_seq in ner_labels]\n",
        "ner_labels = np.array(ner_labels)"
      ],
      "metadata": {
        "id": "qSX-zwyvQqZJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PetPointEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, max_seq_length):\n",
        "        super(PetPointEmbedding, self).__init__()\n",
        "        self.word_embeddings = tf.keras.layers.Embedding(vocab_size, embedding_size, name='word_embeddings')\n",
        "        self.position_embeddings = tf.keras.layers.Embedding(max_seq_length, embedding_size, name='position_embeddings')\n",
        "        self.embedding_hidden_mapping_in = tf.keras.layers.Dense(hidden_size, name='embedding_hidden_mapping_in')\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=1e-12, name='LayerNorm')\n",
        "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "\n",
        "    def call(self, input_ids, training=False):\n",
        "        seq_length = tf.shape(input_ids)[1]\n",
        "        position_ids = tf.range(seq_length)[tf.newaxis, :]\n",
        "\n",
        "        word_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        embeddings = word_embeddings + position_embeddings\n",
        "        embeddings = self.embedding_hidden_mapping_in(embeddings)\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings, training=training)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "tRrEcd7SQt-C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PetPointTransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_size, num_attention_heads, intermediate_size):\n",
        "        super(PetPointTransformerBlock, self).__init__()\n",
        "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_attention_heads, key_dim=hidden_size // num_attention_heads, name='self_attention')\n",
        "        self.attention_dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.attention_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-12)\n",
        "\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(intermediate_size, activation='gelu'),\n",
        "            tf.keras.layers.Dense(hidden_size)\n",
        "        ])\n",
        "        self.ffn_dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.ffn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-12)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attention_output = self.attention(inputs, inputs)\n",
        "        attention_output = self.attention_dropout(attention_output, training=training)\n",
        "        attention_output = self.attention_layer_norm(inputs + attention_output)\n",
        "\n",
        "        ffn_output = self.ffn(attention_output)\n",
        "        ffn_output = self.ffn_dropout(ffn_output, training=training)\n",
        "        ffn_output = self.ffn_layer_norm(attention_output + ffn_output)\n",
        "\n",
        "        return ffn_output"
      ],
      "metadata": {
        "id": "iup56y3lQzHw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PetPointModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, num_hidden_layers, num_attention_heads, intermediate_size, max_seq_length):\n",
        "        super(PetPointModel, self).__init__()\n",
        "        self.embedding = PetPointEmbedding(vocab_size, embedding_size, hidden_size, max_seq_length)\n",
        "        self.encoder = PetPointTransformerBlock(hidden_size, num_attention_heads, intermediate_size)\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "\n",
        "    def call(self, input_ids, training=False):\n",
        "        hidden_states = self.embedding(input_ids, training=training)\n",
        "        for _ in range(self.num_hidden_layers):\n",
        "            hidden_states = self.encoder(hidden_states, training=training)\n",
        "        return hidden_states"
      ],
      "metadata": {
        "id": "ZzqvcNlPQ1_j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PetPointForIntentClassification(tf.keras.Model):\n",
        "    def __init__(self, petpoint_model, num_labels):\n",
        "        super(PetPointForIntentClassification, self).__init__()\n",
        "        self.petpoint = petpoint_model\n",
        "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.classifier = tf.keras.layers.Dense(num_labels, name='classifier')\n",
        "\n",
        "    def call(self, input_ids, training=False):\n",
        "        outputs = self.petpoint(input_ids, training=training)\n",
        "        pooled_output = tf.reduce_mean(outputs, axis=1)\n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "VUZuWU0AQ4hU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PetPointForNER(tf.keras.Model):\n",
        "    def __init__(self, petpoint_model, num_labels):\n",
        "        super(PetPointForNER, self).__init__()\n",
        "        self.petpoint = petpoint_model\n",
        "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.classifier = tf.keras.layers.Dense(num_labels, name='classifier')\n",
        "\n",
        "    def call(self, input_ids, training=False):\n",
        "        outputs = self.petpoint(input_ids, training=training)\n",
        "        sequence_output = self.dropout(outputs, training=training)\n",
        "        logits = self.classifier(sequence_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "GQNdcDw6Q6vv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 128\n",
        "hidden_size = 768\n",
        "num_hidden_layers = 12\n",
        "num_attention_heads = 12\n",
        "intermediate_size = 3072\n",
        "\n",
        "petpoint_model = PetPointModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_size=embedding_size,\n",
        "    hidden_size=hidden_size,\n",
        "    num_hidden_layers=num_hidden_layers,\n",
        "    num_attention_heads=num_attention_heads,\n",
        "    intermediate_size=intermediate_size,\n",
        "    max_seq_length=max_seq_length\n",
        ")"
      ],
      "metadata": {
        "id": "Da1iXCnqQ86q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_intent = PetPointForIntentClassification(petpoint_model, num_labels=num_intents)"
      ],
      "metadata": {
        "id": "fanaTxZkQ_lw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_intent.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "gKuNt21PRCG7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_intent = model_intent.fit(\n",
        "    padded_sequences,\n",
        "    intents_one_hot,\n",
        "    validation_split=0.1,\n",
        "    epochs=5,\n",
        "    batch_size=16\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SytU4tdBREYK",
        "outputId": "941e7534-c049-4895-8e04-0490f706b124"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "24/24 [==============================] - 68s 2s/step - loss: 1.9785 - accuracy: 0.2818 - val_loss: 4.7615 - val_accuracy: 0.2619\n",
            "Epoch 2/5\n",
            "24/24 [==============================] - 55s 2s/step - loss: 1.3797 - accuracy: 0.4634 - val_loss: 4.6949 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "24/24 [==============================] - 53s 2s/step - loss: 0.9690 - accuracy: 0.6802 - val_loss: 4.5706 - val_accuracy: 0.2619\n",
            "Epoch 4/5\n",
            "24/24 [==============================] - 52s 2s/step - loss: 0.7781 - accuracy: 0.7290 - val_loss: 5.1754 - val_accuracy: 0.0952\n",
            "Epoch 5/5\n",
            "24/24 [==============================] - 54s 2s/step - loss: 0.6641 - accuracy: 0.8238 - val_loss: 5.0221 - val_accuracy: 0.2143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ner = PetPointForNER(petpoint_model, num_labels=num_entities)"
      ],
      "metadata": {
        "id": "rP_z5cdKRGgA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ner.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "MpK0vOX6SPZh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_ner = model_ner.fit(\n",
        "    padded_sequences,\n",
        "    ner_labels,\n",
        "    validation_split=0.1,\n",
        "    epochs=5,\n",
        "    batch_size=16\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5-N34VvSSHy",
        "outputId": "979574df-a0e2-42ca-8b2c-92f510747754"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "24/24 [==============================] - 66s 3s/step - loss: 0.1133 - accuracy: 0.9548 - val_loss: 0.0563 - val_accuracy: 0.9762\n",
            "Epoch 2/5\n",
            "24/24 [==============================] - 56s 2s/step - loss: 0.1014 - accuracy: 0.9601 - val_loss: 0.0633 - val_accuracy: 0.9709\n",
            "Epoch 3/5\n",
            "24/24 [==============================] - 55s 2s/step - loss: 0.1003 - accuracy: 0.9606 - val_loss: 0.0361 - val_accuracy: 0.9841\n",
            "Epoch 4/5\n",
            "24/24 [==============================] - 54s 2s/step - loss: 0.0821 - accuracy: 0.9658 - val_loss: 0.0456 - val_accuracy: 0.9828\n",
            "Epoch 5/5\n",
            "24/24 [==============================] - 55s 2s/step - loss: 0.0736 - accuracy: 0.9697 - val_loss: 0.0839 - val_accuracy: 0.9709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model_intent.evaluate(padded_sequences, intents_one_hot)\n",
        "print(f'Akurasi Model Klasifikasi Intent: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6kSDi63SU47",
        "outputId": "b38d81c0-bc1e-48ec-c950-4f0ccf2b58d5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 17s 1s/step - loss: 0.9580 - accuracy: 0.6983\n",
            "Akurasi Model Klasifikasi Intent: 69.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model_ner.evaluate(padded_sequences, ner_labels)\n",
        "print(f'Akurasi Model NER: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iClXZ-i1VIXS",
        "outputId": "8152aa34-0e77-4da2-f749-0afe5e5dfb76"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 16s 1s/step - loss: 0.0700 - accuracy: 0.9709\n",
            "Akurasi Model NER: 97.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FzvzaQu8VNYZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}