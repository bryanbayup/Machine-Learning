{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN3DyVF9drqKi7IyCawywW7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanbayup/Machine-Learning/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim keras-tuner imbalanced-learn Sastrawi sentencepiece seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu_36DqYCW2O",
        "outputId": "03d4c77b-48c1-46f6-c4b5-8fb88fad414c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m857.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=71c706ad119ac9c2c97266e4a15c8b43d058a8d67e97789fd6e698b5c4847ebc\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: Sastrawi, kt-legacy, seqeval, keras-tuner\n",
            "Successfully installed Sastrawi-1.0.1 keras-tuner-1.4.7 kt-legacy-1.0.5 seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download FastText\n",
        "!wget -O id.tar.gz \"https://www.dropbox.com/scl/fi/sju4o3keikox69euw51vy/id.tar.gz?rlkey=5jr3ijtbdwfahq7xcgig28qvy&e=1&st=gntzkzeo&dl=1\"\n",
        "!tar -xzf id.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfaW2FTgMC1V",
        "outputId": "9c11e301-4ffb-49c6-ab6f-3b6d5e602093"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-06 09:01:01--  https://www.dropbox.com/scl/fi/sju4o3keikox69euw51vy/id.tar.gz?rlkey=5jr3ijtbdwfahq7xcgig28qvy&e=1&st=gntzkzeo&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc7e43568fd796229ffc0a4f29d9.dl.dropboxusercontent.com/cd/0/inline/CfsSAnFxfMRmJDQPTYBhu9zlAAV5b5WrkiGbXaVzK7Q7-89aWp4hlxEdvWwjgpIMQQg_3Mf_AM8AlD-UPuwjyCPeI9D5Ky0WvwALlimffXB9Hrn9U1ozZCebcNxM4zqedX0/file?dl=1# [following]\n",
            "--2024-12-06 09:01:02--  https://uc7e43568fd796229ffc0a4f29d9.dl.dropboxusercontent.com/cd/0/inline/CfsSAnFxfMRmJDQPTYBhu9zlAAV5b5WrkiGbXaVzK7Q7-89aWp4hlxEdvWwjgpIMQQg_3Mf_AM8AlD-UPuwjyCPeI9D5Ky0WvwALlimffXB9Hrn9U1ozZCebcNxM4zqedX0/file?dl=1\n",
            "Resolving uc7e43568fd796229ffc0a4f29d9.dl.dropboxusercontent.com (uc7e43568fd796229ffc0a4f29d9.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to uc7e43568fd796229ffc0a4f29d9.dl.dropboxusercontent.com (uc7e43568fd796229ffc0a4f29d9.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CfukARsin3hfFsF5mfbN8VvgDf2mAOhADY6XDpNnbmOozbyAKpqpxjR2c8DdhBK4M-gv2a1K8HZq5nIz1_MNGvm-75X3KGsw3z5Vw6n4_huW0GeDMCTXk1WjJuew3MjAnommS-oMaIpdv3OSFzAzz06uP4lqQaULWDJ9Ls0zCJEOJtiFCRiHUZ0BwTHGcgH1V4lhNjtLo0DEC6gRDvOHlCgagV654VZSAa9tGBf9codpDb8raljA2jgvPPcBCKGuAWlCg6pkQz6S3Hk5004tHUkBJ2lyA-jXzjNUbLDr55Gx4SmHbnCQbkG12qKrqlc4VRYk03ljEZBYCZoZa7FZStIu3j-3ErWjIPRJMnLzVgLcdw/file?dl=1 [following]\n",
            "--2024-12-06 09:01:02--  https://uc7e43568fd796229ffc0a4f29d9.dl.dropboxusercontent.com/cd/0/inline2/CfukARsin3hfFsF5mfbN8VvgDf2mAOhADY6XDpNnbmOozbyAKpqpxjR2c8DdhBK4M-gv2a1K8HZq5nIz1_MNGvm-75X3KGsw3z5Vw6n4_huW0GeDMCTXk1WjJuew3MjAnommS-oMaIpdv3OSFzAzz06uP4lqQaULWDJ9Ls0zCJEOJtiFCRiHUZ0BwTHGcgH1V4lhNjtLo0DEC6gRDvOHlCgagV654VZSAa9tGBf9codpDb8raljA2jgvPPcBCKGuAWlCg6pkQz6S3Hk5004tHUkBJ2lyA-jXzjNUbLDr55Gx4SmHbnCQbkG12qKrqlc4VRYk03ljEZBYCZoZa7FZStIu3j-3ErWjIPRJMnLzVgLcdw/file?dl=1\n",
            "Reusing existing connection to uc7e43568fd796229ffc0a4f29d9.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2333351997 (2.2G) [application/binary]\n",
            "Saving to: ‘id.tar.gz’\n",
            "\n",
            "id.tar.gz           100%[===================>]   2.17G  33.3MB/s    in 62s     \n",
            "\n",
            "2024-12-06 09:02:05 (35.8 MB/s) - ‘id.tar.gz’ saved [2333351997/2333351997]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------\n",
        "# Bagian 1: Instalasi dan Import Library\n",
        "# --------------------------------------\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "import random\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from keras_tuner import HyperModel, RandomSearch\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.corpus import stopwords\n",
        "from seqeval.metrics import classification_report as seq_classification_report\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# --------------------------------------\n",
        "# Bagian 2: Memuat FastText\n",
        "# --------------------------------------\n",
        "try:\n",
        "    fasttext_model = KeyedVectors.load_word2vec_format('id.vec', binary=False)\n",
        "    print(\"FastText 'id.vec' berhasil dimuat.\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal memuat 'id.vec': {e}\")\n",
        "    raise ValueError(\"Gagal memuat FastText.\")\n",
        "\n",
        "# --------------------------------------\n",
        "# Bagian 3: Memuat dataset dari data.json\n",
        "# Pastikan 'data.json' adalah file JSON yang berisi array percakapan seperti yang Anda berikan.\n",
        "# --------------------------------------\n",
        "with open('data2.json', 'r', encoding='utf-8') as f:\n",
        "    conversations = json.load(f)\n",
        "\n",
        "# Data format:\n",
        "# [\n",
        "#   {\n",
        "#       \"conversation_id\": \"...\",\n",
        "#       \"turns\": [\n",
        "#          {\n",
        "#             \"speaker\": \"user\",\n",
        "#             \"utterance\": \"...\",\n",
        "#             \"entities\": [...],\n",
        "#             \"intent\": \"...\"\n",
        "#          },\n",
        "#          {\n",
        "#             \"speaker\": \"bot\",\n",
        "#             \"utterance\": \"...\",\n",
        "#             \"entities\": [...],\n",
        "#             \"intent\": \"...\"\n",
        "#          },\n",
        "#          ...\n",
        "#       ]\n",
        "#   },\n",
        "#   ...\n",
        "# ]\n",
        "\n",
        "# Kita ekstrak hanya turn user, karena kita ingin melatih model intent & NER dari perspektif user input.\n",
        "user_utterances = []\n",
        "intents = []\n",
        "entity_labels = []\n",
        "\n",
        "def char_offset_to_token_labels(utterance, entities, tokenizer=lambda x: x.split()):\n",
        "    tokens = tokenizer(utterance)\n",
        "    labels = [\"O\"] * len(tokens)\n",
        "    # buat mapping token ke character ranges\n",
        "    char_pos = 0\n",
        "    token_ranges = []\n",
        "    for t in tokens:\n",
        "        start_pos = char_pos\n",
        "        end_pos = start_pos + len(t)\n",
        "        token_ranges.append((start_pos, end_pos))\n",
        "        char_pos = end_pos + 1  # spasi\n",
        "\n",
        "    for ent in entities:\n",
        "        ent_start = ent['start']\n",
        "        ent_end = ent['end']\n",
        "        ent_type = ent['entity'].upper()\n",
        "        ent_token_positions = []\n",
        "        for i, (ts, te) in enumerate(token_ranges):\n",
        "            if not (te <= ent_start or ts >= ent_end):\n",
        "                ent_token_positions.append(i)\n",
        "        if len(ent_token_positions) > 0:\n",
        "            labels[ent_token_positions[0]] = \"B-\" + ent_type\n",
        "            for p in ent_token_positions[1:]:\n",
        "                labels[p] = \"I-\" + ent_type\n",
        "    return tokens, labels\n",
        "\n",
        "for conv in conversations:\n",
        "    for turn in conv[\"turns\"]:\n",
        "        if turn[\"speaker\"] == \"user\":\n",
        "            utt = turn[\"utterance\"]\n",
        "            ents = turn.get(\"entities\", [])\n",
        "            intent = turn.get(\"intent\", \"None\")\n",
        "            tokens, ner_tags = char_offset_to_token_labels(utt, ents)\n",
        "            user_utterances.append(tokens)\n",
        "            intents.append(intent)\n",
        "            entity_labels.append(ner_tags)\n",
        "\n",
        "# --------------------------------------\n",
        "# Bagian 4: Preprocessing (Cleaning, Stopwords, Stemming)\n",
        "# --------------------------------------\n",
        "with open('stopword_list_tala.txt', 'r', encoding='utf-8') as f:\n",
        "    stop_words = f.read().splitlines()\n",
        "stop_words = set(word.strip().lower() for word in stop_words)\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = clean_text(text)\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    tokens = [stemmer.stem(w) for w in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Kita akan memproses user_utterances menjadi kalimat lagi untuk TF-IDF nanti\n",
        "utterances_joined = [' '.join(utt) for utt in user_utterances]\n",
        "utterances_clean = [preprocess_text(u) for u in utterances_joined]\n",
        "\n",
        "# --------------------------------------\n",
        "# Bagian 5: Buat DataFrame untuk balancing\n",
        "# Kita perlu intent untuk balancing. Kita akan gunakan oversampling sesuai intent.\n",
        "# --------------------------------------\n",
        "df_data = pd.DataFrame({\n",
        "    'utterances': utterances_joined,\n",
        "    'intent': intents,\n",
        "    'entities': entity_labels,\n",
        "    'utterances_clean': utterances_clean\n",
        "})\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df_data['intent_label'] = label_encoder.fit_transform(df_data['intent'])\n",
        "intent_mapping = dict(zip(df_data['intent_label'], df_data['intent']))\n",
        "\n",
        "# Handling imbalance\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X = df_data.index.values.reshape(-1, 1)\n",
        "y = df_data['intent_label']\n",
        "X_ros, y_ros = ros.fit_resample(X, y)\n",
        "df_balanced = df_data.loc[X_ros.flatten()].reset_index(drop=True)\n",
        "df_balanced['intent_label'] = y_ros\n",
        "df_balanced['intent'] = label_encoder.inverse_transform(df_balanced['intent_label'])\n",
        "\n",
        "# --------------------------------------\n",
        "# Bagian 6: Persiapan data untuk Intent Classification\n",
        "# --------------------------------------\n",
        "texts = df_balanced['utterances_clean'].tolist()\n",
        "labels = df_balanced['intent_label'].tolist()\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts,\n",
        "    labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "max_seq_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in val_sequences))\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_seq_length, padding='post')\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "train_labels_cat = to_categorical(train_labels, num_classes=num_classes)\n",
        "val_labels_cat = to_categorical(val_labels, num_classes=num_classes)\n",
        "\n",
        "# Buat embedding matrix\n",
        "embedding_dim = fasttext_model.vector_size\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, idx in word_index.items():\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix[idx] = fasttext_model[word]\n",
        "    else:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "\n",
        "# --------------------------------------\n",
        "# Bagian 7: Persiapan data NER\n",
        "# Kita harus re-build texts_ner, labels_ner dari df_balanced (yang sudah balanced).\n",
        "# Namun balancing mungkin mengulang sample, hal ini tidak masalah untuk contoh ini.\n",
        "# Yang penting data NER diambil dari df_balanced (karena df_balanced adalah data final).\n",
        "# --------------------------------------\n",
        "# NER label set\n",
        "all_labels = set()\n",
        "for tags in df_balanced['entities']:\n",
        "    for t in tags:\n",
        "        if t != 'O':\n",
        "            all_labels.add(t)\n",
        "all_labels.add('O')\n",
        "all_labels = sorted(list(all_labels))\n",
        "ner_label_encoder = {label: idx for idx, label in enumerate(all_labels)}\n",
        "ner_label_decoder = {idx: label for label, idx in ner_label_encoder.items()}\n",
        "\n",
        "# Encode tags function\n",
        "def encode_tags(tags, max_len):\n",
        "    tag_ids = [ner_label_encoder[t] for t in tags]\n",
        "    tag_ids = tag_ids[:max_len] + [ner_label_encoder['O']]*(max_len - len(tag_ids))\n",
        "    return tag_ids\n",
        "\n",
        "# Kita perlu text sequence untuk NER dari df_balanced\n",
        "# Token ulang berdasarkan tokenizer yang sama\n",
        "def text_to_sequence(text):\n",
        "    # text disini sudah berbentuk token hasil join. Kita gunakan tokenizer.\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    return seq[0]\n",
        "\n",
        "X_ner = []\n",
        "Y_ner = []\n",
        "for i, row in df_balanced.iterrows():\n",
        "    seq = text_to_sequence(row['utterances_clean'])\n",
        "    seq_padded = seq[:max_seq_length] + [0]*(max_seq_length - len(seq))\n",
        "    X_ner.append(seq_padded)\n",
        "    tag_ids = encode_tags(row['entities'], max_seq_length)\n",
        "    Y_ner.append(tag_ids)\n",
        "\n",
        "X_ner = np.array(X_ner)\n",
        "Y_ner = np.array(Y_ner)\n",
        "Y_ner = to_categorical(Y_ner, num_classes=len(ner_label_encoder))\n",
        "\n",
        "# split data untuk NER\n",
        "train_texts_ner, val_texts_ner, train_labels_ner, val_labels_ner = train_test_split(\n",
        "    X_ner,\n",
        "    Y_ner,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# --------------------------------------\n",
        "# Bagian 8: Membangun Model Intent (CNN)\n",
        "# --------------------------------------\n",
        "def build_intent_model_with_cnn(embedding_matrix, max_seq_length, num_classes, l2_reg=0.001):\n",
        "    inputs = Input(shape=(max_seq_length,))\n",
        "    embedding = tf.keras.layers.Embedding(\n",
        "        input_dim=embedding_matrix.shape[0],\n",
        "        output_dim=embedding_matrix.shape[1],\n",
        "        weights=[embedding_matrix],\n",
        "        trainable=True\n",
        "    )(inputs)\n",
        "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedding)\n",
        "    global_pool = GlobalMaxPooling1D()(conv)\n",
        "    dense = Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(global_pool)\n",
        "    dropout = Dropout(0.5)(dense)\n",
        "    outputs = Dense(num_classes, activation='softmax')(dropout)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model_intent_cnn = build_intent_model_with_cnn(embedding_matrix, max_seq_length, num_classes, l2_reg=0.001)\n",
        "model_intent_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "callbacks_intent = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "history_intent_cnn = model_intent_cnn.fit(\n",
        "    train_padded,\n",
        "    train_labels_cat,\n",
        "    validation_data=(val_padded, val_labels_cat),\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    callbacks=callbacks_intent\n",
        ")\n",
        "\n",
        "# --------------------------------------\n",
        "# Bagian 9: Hyperparameter Tuning untuk NER\n",
        "# --------------------------------------\n",
        "class NERHyperModel(HyperModel):\n",
        "    def __init__(self, embedding_matrix, max_seq_length, num_entities):\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.num_entities = num_entities\n",
        "\n",
        "    def build(self, hp):\n",
        "        l2_reg = hp.Choice('l2_reg', values=[1e-4, 1e-3, 1e-2])\n",
        "        dropout_rate = hp.Float('dropout_rate', 0.3, 0.7, step=0.1)\n",
        "        lstm_units = hp.Int('lstm_units', min_value=32, max_value=128, step=32)\n",
        "\n",
        "        inputs = Input(shape=(self.max_seq_length,))\n",
        "        embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=self.embedding_matrix.shape[0],\n",
        "            output_dim=self.embedding_matrix.shape[1],\n",
        "            weights=[self.embedding_matrix],\n",
        "            trainable=True\n",
        "        )(inputs)\n",
        "        lstm = Bidirectional(LSTM(lstm_units, kernel_regularizer=l2(l2_reg), return_sequences=True))(embedding)\n",
        "        dropout = Dropout(dropout_rate)(lstm)\n",
        "        outputs = TimeDistributed(Dense(self.num_entities, activation='softmax'))(dropout)\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "ner_hypermodel = NERHyperModel(embedding_matrix, max_seq_length, len(ner_label_encoder))\n",
        "\n",
        "tuner_ner = RandomSearch(\n",
        "    ner_hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,  # kurangi trials untuk cepat\n",
        "    executions_per_trial=1,\n",
        "    directory='ner_tuner_dir',\n",
        "    project_name='ner_tuning'\n",
        ")\n",
        "\n",
        "tuner_ner.search(\n",
        "    train_texts_ner,\n",
        "    train_labels_ner,\n",
        "    epochs=10,\n",
        "    validation_data=(val_texts_ner, val_labels_ner),\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "best_model_ner = tuner_ner.get_best_models(num_models=1)[0]\n",
        "best_hp_ner = tuner_ner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best Hyperparameters for NER: {best_hp_ner.values}\")\n",
        "\n",
        "loss_intent, accuracy_intent = model_intent_cnn.evaluate(val_padded, val_labels_cat)\n",
        "print(f'Akurasi Model Klasifikasi Intent: {accuracy_intent * 100:.2f}%')\n",
        "\n",
        "loss_ner, accuracy_ner = best_model_ner.evaluate(val_texts_ner, val_labels_ner)\n",
        "print(f'Akurasi Model NER: {accuracy_ner * 100:.2f}%')\n",
        "\n",
        "# --------------------------------------\n",
        "# Bagian 10: Simpan Model dan Encoder\n",
        "# --------------------------------------\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('encoders', exist_ok=True)\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "model_intent_cnn.save('models/model_intent.keras')\n",
        "best_model_ner.save('models/model_ner.keras')\n",
        "\n",
        "with open('encoders/tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('encoders/label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('encoders/ner_label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(ner_label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Buat DataFrame untuk TF-IDF responses\n",
        "# Karena dataset ini tidak punya field \"responses\" dari user,\n",
        "# kita hanya gunakan utterances sebagai contoh. Jika butuh response, siapkan dataset serupa.\n",
        "# Di dataset ini, sebagian besar intent adalah \"Mendiagnosis Gejala\" atau \"Rekomendasi Penanganan Awal\".\n",
        "# Tidak ada explicit 'responses' field dari user. Asumsi: Gunakan utterances sendiri sebagai basis retrieval.\n",
        "df_utterances = df_balanced[['utterances', 'intent', 'utterances_clean']].reset_index(drop=True)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(df_utterances['utterances_clean'])\n",
        "\n",
        "with open('data/vectorizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# --------------------------------------\n",
        "# Bagian 11: intent_animal_mapping\n",
        "# (Dari informasi yang Anda berikan)\n",
        "# --------------------------------------\n",
        "intent_animal_mapping = {\n",
        "    \"Melaporkan Hewan Terlantar\": [\"kucing\", \"anjing\"],\n",
        "    \"Mendiagnosis Gejala\": [\"kucing\", \"anjing\"],\n",
        "    \"Rekomendasi Penanganan Awal\": [\"kucing\", \"anjing\"],\n",
        "    \"Konfirmasi Laporan\": [\"kucing\", \"anjing\"],\n",
        "    \"Tindak Lanjut Laporan\": [\"kucing\", \"anjing\"],\n",
        "    \"Rekomendasi Tindakan\": [\"kucing\", \"anjing\"]\n",
        "}\n",
        "\n",
        "# --------------------------------------\n",
        "# Bagian 12: Fungsi Inference\n",
        "# --------------------------------------\n",
        "def predict_intent(text):\n",
        "    text_clean = preprocess_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "    pred = model_intent_cnn.predict(padded_seq)\n",
        "    predicted_label = np.argmax(pred, axis=1)[0]\n",
        "    intent = label_encoder.inverse_transform([predicted_label])[0]\n",
        "    return intent\n",
        "\n",
        "def predict_entities(text):\n",
        "    text_clean = preprocess_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "    pred = best_model_ner.predict(padded_seq)\n",
        "    pred_labels = np.argmax(pred, axis=-1)[0]\n",
        "    tokens = tokenizer.sequences_to_texts(seq)[0].split()\n",
        "    entities = []\n",
        "    for idx, label_id in enumerate(pred_labels[:len(tokens)]):\n",
        "        label = ner_label_decoder[label_id]\n",
        "        if label != 'O':\n",
        "            entities.append({'entity': label.split('-')[1].lower(), 'value': tokens[idx]})\n",
        "    return entities\n",
        "\n",
        "def adjust_intent(intent, entities):\n",
        "    predicted_animals = intent_animal_mapping.get(intent, None)\n",
        "    entity_animals = [ent['value'].lower() for ent in entities if ent['entity'] == 'animal']\n",
        "    if entity_animals:\n",
        "        user_animal = entity_animals[0]\n",
        "        if predicted_animals and user_animal not in predicted_animals:\n",
        "            # Coba cari intent lain yang cocok dgn animal ini\n",
        "            for i_name, animals in intent_animal_mapping.items():\n",
        "                if user_animal in animals:\n",
        "                    intent = i_name\n",
        "                    break\n",
        "            else:\n",
        "                intent = None\n",
        "    else:\n",
        "        # Jika intent butuh hewan tapi tidak ditemukan, intent bisa diset None atau dibiarkan\n",
        "        # Disini kita biarkan saja\n",
        "        pass\n",
        "    return intent\n",
        "\n",
        "def get_default_response():\n",
        "    default_responses = [\n",
        "        \"Maaf, saya belum bisa menjawab pertanyaan Anda.\",\n",
        "        \"Maaf, mohon diperjelas apa yang Anda maksud.\",\n",
        "        \"Maaf, saya hanya diprogram untuk menjawab pertanyaan mengenai kucing dan anjing.\",\n",
        "        \"Mohon maaf, saya tidak mengerti. Bisa dijelaskan lebih detail?\",\n",
        "        \"Saya belum memiliki informasi mengenai hal tersebut.\"\n",
        "    ]\n",
        "    return random.choice(default_responses)\n",
        "\n",
        "def get_response(user_input, intent=None, entities=None):\n",
        "    user_input_clean = preprocess_text(user_input)\n",
        "    if intent:\n",
        "        df_intent = df_utterances[df_utterances['intent'] == intent]\n",
        "        if df_intent.empty:\n",
        "            return get_default_response()\n",
        "        else:\n",
        "            tfidf_matrix_intent = vectorizer.transform(df_intent['utterances_clean'])\n",
        "            user_tfidf = vectorizer.transform([user_input_clean])\n",
        "            similarities = cosine_similarity(user_tfidf, tfidf_matrix_intent)\n",
        "            most_similar_idx = np.argmax(similarities[0])\n",
        "            highest_similarity = similarities[0][most_similar_idx]\n",
        "            if highest_similarity < 0.2:\n",
        "                return get_default_response()\n",
        "            else:\n",
        "                # Karena dataset ini tidak menyediakan field responses khusus (selain user),\n",
        "                # Anda perlu menyesuaikan. Disini kita akan hanya mengembalikan utterance itu sendiri\n",
        "                # sebagai contoh, atau gunakan get_default_response().\n",
        "                return \"Saya menyarankan untuk konsultasi lebih lanjut ke dokter hewan.\"\n",
        "    else:\n",
        "        return get_default_response()\n",
        "\n",
        "# --------------------------------------\n",
        "# Bagian 13: Multi-Turn Handling\n",
        "# --------------------------------------\n",
        "dialog_history = []\n",
        "MAX_HISTORY = 2\n",
        "\n",
        "def predict_intent_multi_turn(dialog_history, current_input):\n",
        "    history_context = dialog_history[-MAX_HISTORY:] if len(dialog_history) > MAX_HISTORY else dialog_history\n",
        "    combined_input = \" \".join(history_context + [current_input])\n",
        "    text_clean = preprocess_text(combined_input)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "    pred = model_intent_cnn.predict(padded_seq)\n",
        "    predicted_label = np.argmax(pred, axis=1)[0]\n",
        "    intent = label_encoder.inverse_transform([predicted_label])[0]\n",
        "    return intent\n",
        "\n",
        "def predict_entities_multi_turn(dialog_history, current_input):\n",
        "    history_context = dialog_history[-MAX_HISTORY:] if len(dialog_history) > MAX_HISTORY else dialog_history\n",
        "    combined_input = \" \".join(history_context + [current_input])\n",
        "    text_clean = preprocess_text(combined_input)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "    pred = best_model_ner.predict(padded_seq)\n",
        "    pred_labels = np.argmax(pred, axis=-1)[0]\n",
        "    tokens = tokenizer.sequences_to_texts(seq)[0].split()\n",
        "    entities = []\n",
        "    for idx, label_id in enumerate(pred_labels[:len(tokens)]):\n",
        "        label = ner_label_decoder[label_id]\n",
        "        if label != 'O':\n",
        "            entities.append({'entity': label.split('-')[1].lower(), 'value': tokens[idx]})\n",
        "    return entities\n",
        "\n",
        "def chatbot_response_multi_turn(user_input):\n",
        "    dialog_history.append(user_input)\n",
        "    intent = predict_intent_multi_turn(dialog_history, user_input)\n",
        "    entities = predict_entities_multi_turn(dialog_history, user_input)\n",
        "    adjusted_intent = adjust_intent(intent, entities)\n",
        "\n",
        "    if adjusted_intent is None:\n",
        "        response = get_default_response()\n",
        "    else:\n",
        "        response = get_response(user_input, adjusted_intent, entities)\n",
        "        if not response:\n",
        "            response = get_default_response()\n",
        "    return response\n",
        "\n",
        "# --------------------------------------\n",
        "# Bagian 14: Contoh Pengujian Multi-turn\n",
        "# --------------------------------------\n",
        "print(\"=== Simulasi Multi-Turn ===\")\n",
        "input_turns = [\n",
        "    \"Halo, saya punya pertanyaan tentang anjing saya.\",\n",
        "    \"Anjing saya selalu menggaruk telinganya.\",\n",
        "    \"Dia juga ada cairan keluar dari telinganya, apa yang harus saya lakukan?\"\n",
        "]\n",
        "\n",
        "for user_text in input_turns:\n",
        "    print(f\"Anda: {user_text}\")\n",
        "    resp = chatbot_response_multi_turn(user_text)\n",
        "    print(f\"Chatbot: {resp}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTZbrFVvM3UZ",
        "outputId": "08e41639-4ee7-4c34-ab0b-4d43708a6f04"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 13s]\n",
            "val_accuracy: 0.8206349611282349\n",
            "\n",
            "Best val_accuracy So Far: 0.8301587104797363\n",
            "Total elapsed time: 00h 06m 42s\n",
            "Best Hyperparameters for NER: {'l2_reg': 0.0001, 'dropout_rate': 0.4, 'lstm_units': 96}\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9246 - loss: 0.2354 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 20 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi Model Klasifikasi Intent: 94.29%\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8196 - loss: 0.5805 \n",
            "Akurasi Model NER: 83.02%\n",
            "=== Simulasi Multi-Turn ===\n",
            "Anda: Halo, saya punya pertanyaan tentang anjing saya.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 584ms/step\n",
            "Chatbot: Saya menyarankan untuk konsultasi lebih lanjut ke dokter hewan.\n",
            "\n",
            "Anda: Anjing saya selalu menggaruk telinganya.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Chatbot: Saya menyarankan untuk konsultasi lebih lanjut ke dokter hewan.\n",
            "\n",
            "Anda: Dia juga ada cairan keluar dari telinganya, apa yang harus saya lakukan?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Chatbot: Saya menyarankan untuk konsultasi lebih lanjut ke dokter hewan.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Simulasi Multi-Turn ===\")\n",
        "input_turns = [\n",
        "    \"Anjing saya tidak nafsu makan dan sering diare. Apa yang terjadi?\",\n",
        "    \"Tidak, hanya diare saja.\",\n",
        "    \"Terima kasih\"\n",
        "]\n",
        "\n",
        "for user_text in input_turns:\n",
        "    print(f\"Anda: {user_text}\")\n",
        "    resp = chatbot_response_multi_turn(user_text)\n",
        "    print(f\"Chatbot: {resp}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Rws3tXwNVg4",
        "outputId": "cae96fed-f1a2-4d32-a862-8849191d1ca8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Simulasi Multi-Turn ===\n",
            "Anda: Anjing saya tidak nafsu makan dan sering diare. Apa yang terjadi?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Chatbot: Maaf, saya hanya diprogram untuk menjawab pertanyaan mengenai kucing dan anjing.\n",
            "\n",
            "Anda: Tidak, hanya diare saja.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Chatbot: Saya menyarankan untuk konsultasi lebih lanjut ke dokter hewan.\n",
            "\n",
            "Anda: Terima kasih\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Chatbot: Saya menyarankan untuk konsultasi lebih lanjut ke dokter hewan.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xHTFcW6oOS-f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}