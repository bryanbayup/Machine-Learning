{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPbM+pSW/ukzbZiIqQ0E+4I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "77fcfb36056f4136aa57753fda651eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Anda:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_3b8b0d5f2ee241a7a3b026b87c0dce20",
            "placeholder": "Ketik pesan Anda...",
            "style": "IPY_MODEL_e5852a4eaf644b498efc6e1254fc879e",
            "value": ""
          }
        },
        "3b8b0d5f2ee241a7a3b026b87c0dce20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5852a4eaf644b498efc6e1254fc879e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75477cfccf7e458987874631ee088d76": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_8186764241ff43888ba12098d51337b0",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Anda: Bagaimana cara mencegah anjing saya terkena infeksi saluran pencernaan\n",
                  "Chatbot: Bisa jadi tanda kembung atau kondisi serius lainnya. Segera bawa anjing ke dokter hewan untuk evaluasi.\n",
                  "\n",
                  "Informasi tambahan:\n",
                  "Infeksi Parvovirus dapat menyerang saluran pencernaan anjing dan ... Cara terbaik untuk mencegah penularan rabies adalah melakukan vaksinasi pada anjing ...\n",
                  "\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Anda: Apa itu passive income\n",
                  "Chatbot: Untuk mencegah salmonellosis pada kucing, hindari memberikan makanan mentah atau setengah matang, dan pastikan kebersihan lingkungan serta peralatan makan kucing selalu terjaga.\n",
                  "\n",
                  "Informasi tambahan:\n",
                  "Dec 2, 2021 ... Passive income adalah penghasilan yang dilakukan secara pasif. Membangun passive income artinya menyiapkan masa depan agar merdeka secara ...\n",
                  "\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Anda: selamat s\n",
                  "Chatbot: Gunakan sikat gigi kecil dan pasta gigi khusus kucing. Berikan juga makanan atau camilan yang dirancang untuk menjaga kesehatan gigi.\n",
                  "\n",
                  "Informasi tambahan:\n",
                  "Industry 4.0 for the construction industry—how ready is the industry? R Maskuriy, A Selamat, KN Ali, P Maresova, O Krejcar. Applied Sciences 9 (14), 2819, 2019.\n",
                  "\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Anda: selamat sore\n",
                  "Chatbot: Untuk mencegah infeksi cacing tambang pada kucing, berikan obat cacing secara rutin sesuai rekomendasi dokter hewan. Jaga kebersihan lingkungan dan hindari kucing dari area yang terkontaminasi oleh feses hewan lain.\n",
                  "\n",
                  "Informasi tambahan:\n",
                  "Feb 14, 2018 ... Selamat siang is close to Good day and Selamat sore is used when the sun begins to sink (afternoon/dusk) like 3pm to 6pm.\n",
                  "\n"
                ]
              }
            ]
          }
        },
        "8186764241ff43888ba12098d51337b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanbayup/Machine-Learning/blob/main/Untitled16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZQPooANmiDr",
        "outputId": "a695dfb2-8ad5-42f5-9a7f-42ea3a23be4d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=b152d3c5ccad06f9b9ddaf0792fd1e72a33e458fcffb0f0e84305508f98019e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from seqeval.metrics import classification_report as seq_classification_report\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output"
      ],
      "metadata": {
        "id": "X9eoG7SZ5HSQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memuat dataset dari file JSON\n",
        "with open('dataaa.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Mengubah dataset menjadi DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Menentukan jumlah sampel maksimum per intent\n",
        "max_samples = 50\n",
        "\n",
        "df_list = []\n",
        "for intent in df['intent'].unique():\n",
        "    df_intent = df[df['intent'] == intent]\n",
        "    if len(df_intent) > max_samples:\n",
        "        df_intent = resample(df_intent, replace=False, n_samples=max_samples, random_state=42)\n",
        "    df_list.append(df_intent)\n",
        "\n",
        "df_balanced = pd.concat(df_list).reset_index(drop=True)\n",
        "\n",
        "# Periksa distribusi kelas\n",
        "class_counts = df_balanced['intent'].value_counts()\n",
        "print(\"Distribusi kelas sebelum penyesuaian:\")\n",
        "print(class_counts)\n",
        "\n",
        "# Gandakan sampel untuk kelas dengan hanya 1 sampel\n",
        "classes_with_one_sample = class_counts[class_counts == 1].index.tolist()\n",
        "for cls in classes_with_one_sample:\n",
        "    df_class = df_balanced[df_balanced['intent'] == cls]\n",
        "    df_balanced = pd.concat([df_balanced, df_class])  # Menggandakan sampel\n",
        "\n",
        "# Encode intents\n",
        "label_encoder = LabelEncoder()\n",
        "df_balanced['intent_label'] = label_encoder.fit_transform(df_balanced['intent'])\n",
        "\n",
        "# Simpan mapping label untuk penggunaan nanti\n",
        "intent_mapping = dict(zip(df_balanced['intent_label'], df_balanced['intent']))\n",
        "\n",
        "# Memfilter data yang memiliki entitas\n",
        "df_ner = df[df['entities'].map(lambda d: len(d)) > 0].reset_index(drop=True)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JTcg4tL5Icx",
        "outputId": "f4a3feb7-26a5-40bc-e596-4effa27430cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribusi kelas sebelum penyesuaian:\n",
            "intent\n",
            "medical_inquiry_dog       50\n",
            "disease_prevention_cat    50\n",
            "general_conversation      50\n",
            "medical_inquiry_cat       50\n",
            "philosophical_question    42\n",
            "                          ..\n",
            "cat_diet                   1\n",
            "dog_stress_signs           1\n",
            "cat_cleaning               1\n",
            "dog_bathing                1\n",
            "financial_crisis           1\n",
            "Name: count, Length: 64, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Periksa distribusi kelas\n",
        "class_counts = df_balanced['intent'].value_counts()\n",
        "print(\"Distribusi kelas sebelum penyesuaian:\")\n",
        "print(class_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8LdCA5K5bLM",
        "outputId": "7078870c-0bd1-4886-f284-dbd88ae87aba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribusi kelas sebelum penyesuaian:\n",
            "intent\n",
            "medical_inquiry_dog       50\n",
            "disease_prevention_cat    50\n",
            "general_conversation      50\n",
            "medical_inquiry_cat       50\n",
            "philosophical_question    42\n",
            "                          ..\n",
            "cat_diet                   2\n",
            "dog_stress_signs           2\n",
            "cat_cleaning               2\n",
            "dog_bathing                2\n",
            "financial_crisis           2\n",
            "Name: count, Length: 64, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Klasifikasi Intent\n",
        "texts = df_balanced['utterances'].apply(clean_text).tolist()\n",
        "labels = df_balanced['intent_label'].tolist()\n",
        "\n",
        "# Split the data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts,\n",
        "    labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "# Membuat tokenizer\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "# Mengonversi teks ke sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "\n",
        "# Padding sequences\n",
        "max_seq_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in val_sequences))\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_seq_length, padding='post')\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Mengonversi labels ke categorical\n",
        "num_classes = len(label_encoder.classes_)\n",
        "train_labels_cat = to_categorical(train_labels, num_classes=num_classes)\n",
        "val_labels_cat = to_categorical(val_labels, num_classes=num_classes)\n",
        "\n",
        "# NER\n",
        "def prepare_ner_data(df, tokenizer, max_seq_length):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for index, row in df.iterrows():\n",
        "        text = clean_text(row['utterances'])\n",
        "        entities = row['entities']\n",
        "        tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "        label_seq = ['O'] * len(tokens)\n",
        "        for ent in entities:\n",
        "            ent_text = clean_text(ent['value'])\n",
        "            ent_tokens = tokenizer.texts_to_sequences([ent_text])[0]\n",
        "            for i in range(len(tokens)):\n",
        "                if tokens[i:i+len(ent_tokens)] == ent_tokens:\n",
        "                    label_seq[i] = 'B-' + ent['entity']\n",
        "                    for j in range(1, len(ent_tokens)):\n",
        "                        label_seq[i+j] = 'I-' + ent['entity']\n",
        "                    break\n",
        "        texts.append(tokens)\n",
        "        labels.append(label_seq)\n",
        "    # Padding\n",
        "    texts_padded = pad_sequences(texts, maxlen=max_seq_length, padding='post')\n",
        "    return texts_padded, labels\n",
        "\n",
        "# Membuat label encoder untuk NER\n",
        "all_labels = set()\n",
        "for label_list in df_ner['entities']:\n",
        "    for ent in label_list:\n",
        "        all_labels.add('B-' + ent['entity'])\n",
        "        all_labels.add('I-' + ent['entity'])\n",
        "all_labels.add('O')\n",
        "ner_label_encoder = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
        "ner_label_decoder = {idx: label for label, idx in ner_label_encoder.items()}\n",
        "\n",
        "# Siapkan data NER\n",
        "texts_ner = df_ner['utterances'].apply(clean_text).tolist()\n",
        "labels_ner = df_ner['entities'].tolist()\n",
        "\n",
        "texts_ner_prep, labels_ner_prep = prepare_ner_data(df_ner, tokenizer, max_seq_length)\n",
        "\n",
        "# Mengonversi labels ke format numerik\n",
        "def encode_ner_labels(labels, max_seq_length, ner_label_encoder):\n",
        "    labels_encoded = []\n",
        "    for label_seq in labels:\n",
        "        label_ids = [ner_label_encoder[label] for label in label_seq]\n",
        "        label_ids = label_ids + [ner_label_encoder['O']] * (max_seq_length - len(label_ids))\n",
        "        labels_encoded.append(label_ids)\n",
        "    labels_encoded = np.array(labels_encoded)\n",
        "    labels_encoded = to_categorical(labels_encoded, num_classes=len(ner_label_encoder))\n",
        "    return labels_encoded\n",
        "\n",
        "labels_ner_encoded = encode_ner_labels(labels_ner_prep, max_seq_length, ner_label_encoder)\n",
        "\n",
        "# Split the data\n",
        "train_texts_ner, val_texts_ner, train_labels_ner, val_labels_ner = train_test_split(\n",
        "    texts_ner_prep,\n",
        "    labels_ner_encoded,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "9b0Ka3qx5NZE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisi Fungsi Attention\n",
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1),\n",
        "                                 initializer='glorot_uniform', trainable=True)\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        e = K.tanh(K.dot(x, self.W))\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = x * a\n",
        "        return K.sum(output, axis=1)\n",
        "\n",
        "# Membangun Model\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Embedding, Dense, Input, Bidirectional, LSTM, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Definisikan input\n",
        "inputs = Input(shape=(max_seq_length,))\n",
        "\n",
        "# Embedding Layer\n",
        "embedding = Embedding(input_dim=vocab_size, output_dim=256, input_length=max_seq_length)(inputs)\n",
        "\n",
        "# Convolutional Layer\n",
        "conv = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding)\n",
        "conv = GlobalMaxPooling1D()(conv)\n",
        "\n",
        "# BiLSTM Layer dengan Attention\n",
        "lstm = Bidirectional(LSTM(128, return_sequences=True))(embedding)\n",
        "attention = AttentionLayer()(lstm)\n",
        "\n",
        "# Menggabungkan fitur dari CNN dan LSTM\n",
        "combined = tf.keras.layers.concatenate([conv, attention])\n",
        "\n",
        "# Fully Connected Layer\n",
        "dense = Dense(64, activation='relu')(combined)\n",
        "dropout = Dropout(0.5)(dense)\n",
        "outputs = Dense(num_classes, activation='softmax')(dropout)\n",
        "\n",
        "# Membangun Model\n",
        "model_intent = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Kompilasi Model\n",
        "model_intent.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Melatih Model\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    ModelCheckpoint(\n",
        "        filepath='best_model_intent.weights.h5',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "history_intent = model_intent.fit(\n",
        "    train_padded,\n",
        "    train_labels_cat,\n",
        "    validation_data=(val_padded, val_labels_cat),\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9yiHA945cT2",
        "outputId": "9192755f-b295-4970-bf27-b6b82154024f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - accuracy: 0.0463 - loss: 4.0618 - val_accuracy: 0.1709 - val_loss: 3.4759\n",
            "Epoch 2/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.2115 - loss: 3.3558 - val_accuracy: 0.4530 - val_loss: 2.4702\n",
            "Epoch 3/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.3049 - loss: 2.7167 - val_accuracy: 0.5385 - val_loss: 2.0829\n",
            "Epoch 4/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 69ms/step - accuracy: 0.4464 - loss: 2.3232 - val_accuracy: 0.6410 - val_loss: 1.8222\n",
            "Epoch 5/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.5747 - loss: 1.8413 - val_accuracy: 0.6667 - val_loss: 1.6255\n",
            "Epoch 6/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.6171 - loss: 1.7245 - val_accuracy: 0.6752 - val_loss: 1.4475\n",
            "Epoch 7/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - accuracy: 0.6425 - loss: 1.4369 - val_accuracy: 0.6838 - val_loss: 1.3203\n",
            "Epoch 8/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - accuracy: 0.6678 - loss: 1.3810 - val_accuracy: 0.6923 - val_loss: 1.2310\n",
            "Epoch 9/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.7116 - loss: 1.1203 - val_accuracy: 0.7094 - val_loss: 1.1852\n",
            "Epoch 10/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.7091 - loss: 1.0611 - val_accuracy: 0.7179 - val_loss: 1.1386\n",
            "Epoch 11/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.7587 - loss: 0.9130 - val_accuracy: 0.7179 - val_loss: 1.0986\n",
            "Epoch 12/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.8011 - loss: 0.7308 - val_accuracy: 0.7436 - val_loss: 1.0280\n",
            "Epoch 13/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.7748 - loss: 0.7452 - val_accuracy: 0.7265 - val_loss: 1.0244\n",
            "Epoch 14/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.7990 - loss: 0.7146 - val_accuracy: 0.7436 - val_loss: 0.9794\n",
            "Epoch 15/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.8311 - loss: 0.5619 - val_accuracy: 0.7521 - val_loss: 0.9053\n",
            "Epoch 16/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.8182 - loss: 0.6290 - val_accuracy: 0.7521 - val_loss: 0.9117\n",
            "Epoch 17/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.8528 - loss: 0.5409 - val_accuracy: 0.7692 - val_loss: 0.8696\n",
            "Epoch 18/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.8537 - loss: 0.4837 - val_accuracy: 0.7949 - val_loss: 0.8056\n",
            "Epoch 19/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 67ms/step - accuracy: 0.8823 - loss: 0.4461 - val_accuracy: 0.7949 - val_loss: 0.8328\n",
            "Epoch 20/20\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.9123 - loss: 0.3241 - val_accuracy: 0.8120 - val_loss: 0.7567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisi Fungsi Attention\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "\n",
        "class TimeDistributedAttention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(TimeDistributedAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], input_shape[-1]),\n",
        "                                 initializer='glorot_uniform', trainable=True)\n",
        "        self.b = self.add_weight(name='att_bias', shape=(input_shape[-1],),\n",
        "                                 initializer='zeros', trainable=True)\n",
        "        super(TimeDistributedAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = x * a\n",
        "        return output\n",
        "\n",
        "# Membangun Model\n",
        "from tensorflow.keras.layers import TimeDistributed, Bidirectional, Dense, LSTM, Embedding, Dropout\n",
        "\n",
        "# Definisikan input\n",
        "inputs = Input(shape=(max_seq_length,))\n",
        "\n",
        "# Embedding Layer\n",
        "embedding = Embedding(input_dim=vocab_size, output_dim=256, input_length=max_seq_length)(inputs)\n",
        "\n",
        "# BiLSTM Layer dengan Attention\n",
        "lstm = Bidirectional(LSTM(128, return_sequences=True))(embedding)\n",
        "attention = TimeDistributedAttention()(lstm)\n",
        "\n",
        "# TimeDistributed Dense Layer\n",
        "outputs = TimeDistributed(Dense(len(ner_label_encoder), activation='softmax'))(attention)\n",
        "\n",
        "# Membangun Model\n",
        "model_ner = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Kompilasi Model\n",
        "model_ner.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Melatih Model\n",
        "callbacks_ner = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    ModelCheckpoint(\n",
        "        filepath='best_model_ner.weights.h5',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "history_ner = model_ner.fit(\n",
        "    train_texts_ner,\n",
        "    train_labels_ner,\n",
        "    validation_data=(val_texts_ner, val_labels_ner),\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    callbacks=callbacks_ner\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjMKQzlo5geF",
        "outputId": "05f70748-b367-46c3-c2f7-22159893f594"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 111ms/step - accuracy: 0.6115 - loss: 3.0862 - val_accuracy: 0.7925 - val_loss: 2.6420\n",
            "Epoch 2/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.7588 - loss: 2.4812 - val_accuracy: 0.7925 - val_loss: 2.0017\n",
            "Epoch 3/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.7793 - loss: 1.9193 - val_accuracy: 0.7925 - val_loss: 1.6538\n",
            "Epoch 4/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7673 - loss: 1.6076 - val_accuracy: 0.7925 - val_loss: 1.4055\n",
            "Epoch 5/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.7587 - loss: 1.3974 - val_accuracy: 0.7925 - val_loss: 1.2212\n",
            "Epoch 6/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 193ms/step - accuracy: 0.7525 - loss: 1.2308 - val_accuracy: 0.7925 - val_loss: 1.0794\n",
            "Epoch 7/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.7741 - loss: 1.0701 - val_accuracy: 0.7925 - val_loss: 0.9657\n",
            "Epoch 8/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7715 - loss: 0.9655 - val_accuracy: 0.7925 - val_loss: 0.8810\n",
            "Epoch 9/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.7763 - loss: 0.8854 - val_accuracy: 0.7925 - val_loss: 0.8179\n",
            "Epoch 10/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.7684 - loss: 0.8331 - val_accuracy: 0.7925 - val_loss: 0.7682\n",
            "Epoch 11/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.7750 - loss: 0.7660 - val_accuracy: 0.8124 - val_loss: 0.7278\n",
            "Epoch 12/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8012 - loss: 0.7256 - val_accuracy: 0.8229 - val_loss: 0.6968\n",
            "Epoch 13/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8027 - loss: 0.6883 - val_accuracy: 0.8239 - val_loss: 0.6683\n",
            "Epoch 14/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.7983 - loss: 0.6797 - val_accuracy: 0.8239 - val_loss: 0.6445\n",
            "Epoch 15/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.8068 - loss: 0.6421 - val_accuracy: 0.8239 - val_loss: 0.6247\n",
            "Epoch 16/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 151ms/step - accuracy: 0.8038 - loss: 0.6144 - val_accuracy: 0.8239 - val_loss: 0.6032\n",
            "Epoch 17/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 175ms/step - accuracy: 0.7969 - loss: 0.5898 - val_accuracy: 0.8229 - val_loss: 0.5893\n",
            "Epoch 18/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 203ms/step - accuracy: 0.8030 - loss: 0.5821 - val_accuracy: 0.8239 - val_loss: 0.5712\n",
            "Epoch 19/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 182ms/step - accuracy: 0.8061 - loss: 0.5624 - val_accuracy: 0.8323 - val_loss: 0.5588\n",
            "Epoch 20/20\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - accuracy: 0.8327 - loss: 0.5284 - val_accuracy: 0.8396 - val_loss: 0.5437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best weights\n",
        "model_intent.load_weights('best_model_intent.weights.h5')\n",
        "\n",
        "# Evaluasi\n",
        "loss, accuracy = model_intent.evaluate(val_padded, val_labels_cat)\n",
        "print(f'Akurasi Model Klasifikasi Intent: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Predict on validation data\n",
        "val_preds = model_intent.predict(val_padded)\n",
        "val_preds = np.argmax(val_preds, axis=1)\n",
        "val_true = np.argmax(val_labels_cat, axis=1)\n",
        "\n",
        "# Get unique labels present in val_true\n",
        "unique_labels = np.unique(val_true)\n",
        "\n",
        "# Filter label_encoder.classes_ to include only the present labels\n",
        "target_names = [label_encoder.classes_[i] for i in unique_labels]\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(val_true, val_preds, labels=unique_labels, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kK80vJbd56Zd",
        "outputId": "7966cfe8-91a3-49de-908b-63c790b23946"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8133 - loss: 0.7037\n",
            "Akurasi Model Klasifikasi Intent: 81.20%\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374ms/step\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "       bot_capabilities       1.00      1.00      1.00         1\n",
            "            bot_emotion       0.00      0.00      0.00         1\n",
            "       business_startup       0.00      0.00      0.00         1\n",
            "           cat_cleaning       1.00      1.00      1.00         1\n",
            "               cat_diet       1.00      1.00      1.00         1\n",
            "         cat_healthcare       1.00      1.00      1.00         6\n",
            "        debt_management       0.00      0.00      0.00         1\n",
            " disease_prevention_cat       1.00      1.00      1.00        10\n",
            " disease_prevention_dog       0.88      0.88      0.88         8\n",
            "            dog_bathing       0.00      0.00      0.00         1\n",
            "dog_behavior_management       1.00      1.00      1.00         1\n",
            "  dog_biting_prevention       1.00      1.00      1.00         1\n",
            "         dog_healthcare       0.88      0.88      0.88         8\n",
            "       financial_basics       0.60      0.75      0.67         4\n",
            "       financial_crisis       1.00      1.00      1.00         1\n",
            "   general_conversation       1.00      0.90      0.95        10\n",
            "               greeting       0.43      1.00      0.60         3\n",
            "        investment_tips       0.00      0.00      0.00         1\n",
            "            kitten_care       1.00      1.00      1.00         1\n",
            "     long_hair_cat_care       1.00      1.00      1.00         1\n",
            "    medical_inquiry_cat       1.00      1.00      1.00        10\n",
            "    medical_inquiry_dog       0.91      1.00      0.95        10\n",
            "       personal_finance       0.00      0.00      0.00         1\n",
            "pet_health_status_check       0.00      0.00      0.00         2\n",
            " philosophical_question       0.82      1.00      0.90         9\n",
            "         puppy_training       0.00      0.00      0.00         1\n",
            "            saving_tips       0.00      0.00      0.00         1\n",
            "   science_conversation       1.00      0.62      0.77         8\n",
            "   symptom_analysis_cat       0.80      1.00      0.89         4\n",
            "   symptom_analysis_dog       0.75      0.75      0.75         4\n",
            "             user_anger       0.00      0.00      0.00         1\n",
            "         user_apologize       0.00      0.00      0.00         1\n",
            "         user_complaint       0.00      0.00      0.00         1\n",
            "         user_giving_up       0.00      0.00      0.00         1\n",
            "          user_thankful       0.50      1.00      0.67         1\n",
            "\n",
            "              micro avg       0.84      0.81      0.83       117\n",
            "              macro avg       0.56      0.59      0.57       117\n",
            "           weighted avg       0.79      0.81      0.79       117\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best weights\n",
        "model_ner.load_weights('best_model_ner.weights.h5')\n",
        "\n",
        "# Evaluasi\n",
        "loss, accuracy = model_ner.evaluate(val_texts_ner, val_labels_ner)\n",
        "print(f'Akurasi Model NER: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Predict on validation data\n",
        "val_preds = model_ner.predict(val_texts_ner)\n",
        "val_preds = np.argmax(val_preds, axis=-1)\n",
        "val_true = np.argmax(val_labels_ner, axis=-1)\n",
        "\n",
        "# Konversi label ke format aslinya\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for i in range(len(val_preds)):\n",
        "    true_label = []\n",
        "    pred_label = []\n",
        "    for j in range(len(val_preds[i])):\n",
        "        true_l = ner_label_decoder[val_true[i][j]]\n",
        "        pred_l = ner_label_decoder[val_preds[i][j]]\n",
        "        if true_l != 'O':\n",
        "            true_label.append(true_l)\n",
        "            pred_label.append(pred_l)\n",
        "    true_labels.append(true_label)\n",
        "    pred_labels.append(pred_label)\n",
        "\n",
        "# Classification report\n",
        "print(seq_classification_report(true_labels, pred_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwbqW9Q76dFV",
        "outputId": "42f19d01-57c5-4547-a96b-f875344de65d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8404 - loss: 0.5480\n",
            "Akurasi Model NER: 83.96%\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 808ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      animal       1.00      0.89      0.94        35\n",
            "    behavior       0.00      0.00      0.00         1\n",
            "     disease       0.00      0.00      0.00        19\n",
            "    duration       0.00      0.00      0.00         2\n",
            "        food       0.00      0.00      0.00         1\n",
            "     symptom       0.11      0.12      0.12        42\n",
            "\n",
            "   micro avg       0.48      0.36      0.41       100\n",
            "   macro avg       0.19      0.17      0.18       100\n",
            "weighted avg       0.40      0.36      0.38       100\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model dan tokenizer\n",
        "model_intent.save('model_intent.h5')\n",
        "model_ner.save('model_ner.h5')\n",
        "\n",
        "import pickle\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('ner_label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(ner_label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Memuat model dan tokenizer\n",
        "from tensorflow.keras.models import load_model\n",
        "model_intent = load_model('model_intent.h5', custom_objects={'AttentionLayer': AttentionLayer})\n",
        "model_ner = load_model('model_ner.h5', custom_objects={'TimeDistributedAttention': TimeDistributedAttention})\n",
        "\n",
        "import pickle\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "with open('label_encoder.pickle', 'rb') as handle:\n",
        "    label_encoder = pickle.load(handle)\n",
        "with open('ner_label_encoder.pickle', 'rb') as handle:\n",
        "    ner_label_encoder = pickle.load(handle)\n",
        "ner_label_decoder = {idx: label for label, idx in ner_label_encoder.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtWTjDl26nOV",
        "outputId": "c5712a54-86b1-4d4f-ddb2-1606448a84df"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_intent(text):\n",
        "    text_clean = clean_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "    pred = model_intent.predict(padded_seq)\n",
        "    predicted_label = np.argmax(pred, axis=1)[0]\n",
        "    intent = label_encoder.inverse_transform([predicted_label])[0]\n",
        "    return intent\n",
        "\n",
        "def predict_entities(text):\n",
        "    text_clean = clean_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "    pred = model_ner.predict(padded_seq)\n",
        "    pred_labels = np.argmax(pred, axis=-1)[0]\n",
        "    tokens = tokenizer.sequences_to_texts(seq)[0].split()\n",
        "    entities = []\n",
        "    for idx, label_id in enumerate(pred_labels[:len(tokens)]):\n",
        "        label = ner_label_decoder[label_id]\n",
        "        if label != 'O':\n",
        "            entities.append({'entity': label.split('-')[1], 'value': tokens[idx]})\n",
        "    return entities"
      ],
      "metadata": {
        "id": "Gg0q8LGn6sRZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pastikan model telah dipanggil\n",
        "_ = model_intent.predict(train_padded[:1])\n",
        "\n",
        "# Mendefinisikan model tanpa layer output untuk mendapatkan embedding\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "embedding_model = Model(inputs=model_intent.input, outputs=model_intent.get_layer('dense_7').output)\n",
        "\n",
        "# Mendapatkan embedding dari dataset\n",
        "utterance_embeddings = embedding_model.predict(train_padded)\n",
        "\n",
        "# Fungsi get_response\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def get_response(user_input):\n",
        "    # Preprocess input\n",
        "    text_clean = clean_text(user_input)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "    # Mendapatkan embedding input pengguna\n",
        "    user_embedding = embedding_model.predict(padded_seq)\n",
        "\n",
        "    # Menghitung kemiripan\n",
        "    similarities = cosine_similarity(user_embedding, utterance_embeddings)\n",
        "\n",
        "    # Dapatkan indeks dengan similarity tertinggi\n",
        "    most_similar_idx = np.argmax(similarities[0])\n",
        "\n",
        "    # Jika similarity rendah, berikan respon default\n",
        "    if similarities[0][most_similar_idx] < 0.5:\n",
        "        return \"Maaf, saya tidak memahami pertanyaan Anda.\"\n",
        "\n",
        "    # Ambil respon yang sesuai\n",
        "    response = df_balanced.iloc[most_similar_idx]['responses']\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0lsBKZ_6viK",
        "outputId": "1f30f046-ec28-471f-8dc6-76f26f8ffacc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7cbaa2896dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "\n",
        "def google_search(query):\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=\"AIzaSyD-f-0S98J0bcdG_5AvbuNGSBVHIgQIX1Y\")\n",
        "    res = service.cse().list(q=query, cx='30b6a924536894083').execute()\n",
        "    results = res.get('items', [])\n",
        "    if results:\n",
        "        snippet = results[0]['snippet']\n",
        "        return snippet\n",
        "    else:\n",
        "        return \"Maaf, saya tidak menemukan informasi yang Anda cari.\""
      ],
      "metadata": {
        "id": "lbJBuVc97mQg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_response(user_input):\n",
        "    # Prediksi intent dan entitas\n",
        "    intent = predict_intent(user_input)\n",
        "    entities = predict_entities(user_input)\n",
        "\n",
        "    # Jika intent tidak dikenali, gunakan Google Search\n",
        "    if intent == 'general_conversation':\n",
        "        response = get_response(user_input)\n",
        "    else:\n",
        "        # Gunakan Google Search untuk mendapatkan informasi tambahan\n",
        "        query = user_input\n",
        "        search_result = google_search(query)\n",
        "        response = f\"{get_response(user_input)}\\n\\nInformasi tambahan:\\n{search_result}\"\n",
        "\n",
        "    return response\n",
        "\n",
        "# Membuat widget input dan output\n",
        "input_box = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Ketik pesan Anda...',\n",
        "    description='Anda:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def on_submit(sender):\n",
        "    user_input = input_box.value\n",
        "    input_box.value = ''\n",
        "    response = chatbot_response(user_input)\n",
        "    with output_area:\n",
        "        print(f\"Anda: {user_input}\")\n",
        "        print(f\"Chatbot: {response}\\n\")\n",
        "\n",
        "input_box.on_submit(on_submit)\n",
        "\n",
        "display(input_box, output_area)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762,
          "referenced_widgets": [
            "77fcfb36056f4136aa57753fda651eb4",
            "3b8b0d5f2ee241a7a3b026b87c0dce20",
            "e5852a4eaf644b498efc6e1254fc879e",
            "75477cfccf7e458987874631ee088d76",
            "8186764241ff43888ba12098d51337b0"
          ]
        },
        "id": "9tSXLpbZ6yAV",
        "outputId": "f19c1927-d2ec-43cc-b9e5-353274ff5b68"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Anda:', placeholder='Ketik pesan Anda...')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77fcfb36056f4136aa57753fda651eb4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75477cfccf7e458987874631ee088d76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh kalimat\n",
        "test_sentence = \"Kucing saya sering muntah dan tidak mau makan.\"\n",
        "\n",
        "# Prediksi intent\n",
        "predicted_intent = predict_intent(test_sentence)\n",
        "print(f\"Intent yang diprediksi: {predicted_intent}\")\n",
        "\n",
        "# Prediksi entitas\n",
        "predicted_entities = predict_entities(test_sentence)\n",
        "print(\"Entitas yang ditemukan:\")\n",
        "for entity in predicted_entities:\n",
        "    print(f\"- {entity['entity']}: {entity['value']}\")\n",
        "\n",
        "# Mendapatkan respon\n",
        "response = chatbot_response(test_sentence)\n",
        "print(f\"Chatbot: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ju9PCLo7KsW",
        "outputId": "39b44375-4d5d-416a-d9ca-7969dd59b0a7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Intent yang diprediksi: symptom_analysis_cat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Entitas yang ditemukan:\n",
            "- animal: kucing\n",
            "- symptom: sering\n",
            "- symptom: muntah\n",
            "- symptom: mau\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Chatbot: Anjing dewasa sebaiknya diperiksa setidaknya sekali setahun, sementara anak anjing dan anjing senior memerlukan pemeriksaan lebih sering.\n",
            "\n",
            "Informasi tambahan:\n",
            "Kucing muntah sebenarnya wajar dan terjadi bisa terjadi sebagai respons tubuh, misalnya saat keracunan makanan atau kemasukan benda asing. Kucing juga bisa ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B4LMgNwQ7Nlr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}