{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNUjlGV0q3w10vwq/z9sbJW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanbayup/Machine-Learning/blob/main/Untitled19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.11.0\n",
        "!pip install seqeval\n",
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZQPooANmiDr",
        "outputId": "93cfa46d-02ca-4808-9c36-aa6ee535e509"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.11.0 in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.67.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.12.1)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.2)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.45.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
            "Collecting seqeval\n",
            "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=f41906617780cd55d0c203bcb99de118db677a8b703d75d9580bc053203de2de\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, TimeDistributed, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow_addons as tfa\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow_addons.text import crf_log_likelihood, crf_decode\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXGwKz_VWA1E",
        "outputId": "d6c1074c-9f7f-42f8-fabf-05cd07ce822b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset from dataaa.json\n",
        "with open('dataaa.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display first few entries\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtL885VtWotz",
        "outputId": "19094b61-605f-4629-954b-982d6e9e24b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          utterances               intent  \\\n",
            "0  Anjing saya mengalami gatal-gatal terus meneru...  medical_inquiry_dog   \n",
            "1  Anjing saya terlihat sering muntah dan kehilan...  medical_inquiry_dog   \n",
            "2  Anjing saya terlihat lesu, demam, dan tidak ma...  medical_inquiry_dog   \n",
            "3  Anjing saya mengalami batuk kering dan nafasny...  medical_inquiry_dog   \n",
            "4  Anjing saya mengalami luka pada kulit yang men...  medical_inquiry_dog   \n",
            "\n",
            "                                            entities  \\\n",
            "0  [{'entity': 'animal', 'value': 'Anjing', 'star...   \n",
            "1  [{'entity': 'animal', 'value': 'Anjing', 'star...   \n",
            "2  [{'entity': 'animal', 'value': 'Anjing', 'star...   \n",
            "3  [{'entity': 'animal', 'value': 'Anjing', 'star...   \n",
            "4  [{'entity': 'animal', 'value': 'Anjing', 'star...   \n",
            "\n",
            "                                           responses  \n",
            "0  Gunakan sampo hipoalergenik, oleskan salep hid...  \n",
            "1  Berikan cairan elektrolit untuk mencegah dehid...  \n",
            "2  Pastikan anjing tetap terhidrasi, gunakan komp...  \n",
            "3  Berikan obat batuk khusus anjing yang disarank...  \n",
            "4  Bersihkan luka dengan antiseptik, oleskan sale...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text"
      ],
      "metadata": {
        "id": "PBmMeOIMWvfT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Cleaning to Utterances\n",
        "df['clean_utterances'] = df['utterances'].apply(clean_text)\n",
        "\n",
        "# Encode Intents\n",
        "label_encoder = LabelEncoder()\n",
        "df['intent_label'] = label_encoder.fit_transform(df['intent'])\n",
        "num_intent_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Prepare Entities for NER\n",
        "all_labels = set(['O'])\n",
        "for label_list in df['entities']:\n",
        "    for ent in label_list:\n",
        "        all_labels.add('B-' + ent['entity'])\n",
        "        all_labels.add('I-' + ent['entity'])\n",
        "\n",
        "# Create tag2idx and idx2tag mappings\n",
        "tag2idx = {tag: idx for idx, tag in enumerate(sorted(all_labels))}\n",
        "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
        "num_tags = len(tag2idx)\n",
        "\n",
        "# Initialize Tokenizer\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['clean_utterances'])\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1  # +1 for padding\n",
        "\n",
        "# Convert Utterances to Sequences\n",
        "sequences = tokenizer.texts_to_sequences(df['clean_utterances'])\n",
        "\n",
        "# Determine Maximum Sequence Length\n",
        "max_seq_length = max(len(seq) for seq in sequences)\n",
        "\n",
        "# Pad Sequences\n",
        "X = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Prepare NER Labels\n",
        "def prepare_ner_labels(entities, tokens, tag2idx):\n",
        "    labels = ['O'] * len(tokens)\n",
        "    for ent in entities:\n",
        "        entity = ent['entity']\n",
        "        value = ent['value']\n",
        "        # Tokenize entity value\n",
        "        ent_text_clean = clean_text(value)\n",
        "        ent_tokens = ent_text_clean.split()\n",
        "        ent_length = len(ent_tokens)\n",
        "        for i in range(len(tokens) - ent_length + 1):\n",
        "            if tokens[i:i+ent_length] == ent_tokens:\n",
        "                labels[i] = 'B-' + entity\n",
        "                for j in range(1, ent_length):\n",
        "                    labels[i + j] = 'I-' + entity\n",
        "                break  # Avoid overlapping\n",
        "    # Convert labels to indices and pad\n",
        "    label_ids = [tag2idx.get(label, tag2idx['O']) for label in labels]\n",
        "    label_ids += [tag2idx['O']] * (max_seq_length - len(label_ids))\n",
        "    return label_ids[:max_seq_length]\n",
        "\n",
        "# Apply to All Entries\n",
        "y_ner = []\n",
        "for idx, row in df.iterrows():\n",
        "    tokens = df['clean_utterances'].iloc[idx].split()\n",
        "    label_ids = prepare_ner_labels(row['entities'], tokens, tag2idx)\n",
        "    y_ner.append(label_ids)\n",
        "\n",
        "y_ner = np.array(y_ner)"
      ],
      "metadata": {
        "id": "DzdYzgNUWxuL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Data into Train and Test for Intent Classification\n",
        "X_train_intent, X_test_intent, y_train_intent, y_test_intent = train_test_split(\n",
        "    X, df['intent_label'], test_size=0.1, random_state=42, stratify=df['intent_label']\n",
        ")\n",
        "\n",
        "# Split Data into Train and Test for NER\n",
        "X_train_ner, X_test_ner, y_train_ner, y_test_ner = train_test_split(\n",
        "    X, y_ner, test_size=0.1, random_state=42, stratify=df['intent_label']\n",
        ")\n",
        "\n",
        "print(\"Intent Classification:\")\n",
        "print(\"Train:\", X_train_intent.shape, y_train_intent.shape)\n",
        "print(\"Test:\", X_test_intent.shape, y_test_intent.shape)\n",
        "\n",
        "print(\"\\nNER:\")\n",
        "print(\"Train:\", X_train_ner.shape, y_train_ner.shape)\n",
        "print(\"Test:\", X_test_ner.shape, y_test_ner.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsaT-oIeWzvJ",
        "outputId": "26059569-0361-4797-da9b-8f2ca049f926"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent Classification:\n",
            "Train: (511, 19) (511,)\n",
            "Test: (57, 19) (57,)\n",
            "\n",
            "NER:\n",
            "Train: (511, 19) (511, 19)\n",
            "Test: (57, 19) (57, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define Intent Classification Model\n",
        "def build_intent_model(vocab_size, embedding_dim, lstm_units, num_classes, max_length):\n",
        "    inputs = Input(shape=(max_length,), dtype='int32')\n",
        "    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length, mask_zero=True)(inputs)\n",
        "    bilstm = Bidirectional(LSTM(units=lstm_units))(embedding)\n",
        "    dropout = Dropout(0.5)(bilstm)\n",
        "    dense = Dense(64, activation='relu')(dropout)\n",
        "    outputs = Dense(num_classes, activation='softmax')(dense)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim_intent = 128\n",
        "lstm_units_intent = 64\n",
        "\n",
        "# Build Intent Model\n",
        "model_intent = build_intent_model(vocab_size, embedding_dim_intent, lstm_units_intent, num_intent_classes, max_seq_length)\n",
        "model_intent.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9Q9GdiGW3wY",
        "outputId": "ad261374-130b-4da5-e7ec-8a85c317e2d8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 19)]              0         \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, 19, 128)           98816     \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 128)              98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 16)                1040      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 206,928\n",
            "Trainable params: 206,928\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, TimeDistributed, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow_addons.text import crf_log_likelihood, crf_decode\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "79h0zQoeYtin"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom CRF Layer\n",
        "class CRFLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_tags, name=\"crf_layer\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.num_tags = num_tags\n",
        "        self.transition_params = tf.Variable(\n",
        "            tf.random.uniform(shape=(num_tags, num_tags)), trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, seq_lengths):\n",
        "        # Decode the highest scoring sequence\n",
        "        decoded_sequence, _ = crf_decode(\n",
        "            inputs, self.transition_params, seq_lengths\n",
        "        )\n",
        "        return decoded_sequence\n",
        "\n",
        "    def crf_loss(self, y_true, y_pred, seq_lengths):\n",
        "        log_likelihood, _ = crf_log_likelihood(\n",
        "            y_pred, y_true, seq_lengths, self.transition_params\n",
        "        )\n",
        "        return -tf.reduce_mean(log_likelihood)\n",
        "\n",
        "\n",
        "# Define NER Model\n",
        "def build_ner_model(vocab_size, embedding_dim, lstm_units, num_tags, max_length):\n",
        "    inputs = Input(shape=(max_length,), dtype=\"int32\")\n",
        "    seq_lengths = tf.reduce_sum(tf.cast(inputs != 0, tf.int32), axis=-1)\n",
        "\n",
        "    # Embedding layer\n",
        "    embedding = Embedding(\n",
        "        input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length, mask_zero=True\n",
        "    )(inputs)\n",
        "\n",
        "    # BiLSTM layer\n",
        "    bilstm = Bidirectional(LSTM(units=lstm_units, return_sequences=True))(embedding)\n",
        "    dropout = Dropout(0.5)(bilstm)\n",
        "    dense = TimeDistributed(Dense(num_tags))(dropout)\n",
        "\n",
        "    # CRF Layer\n",
        "    crf = CRFLayer(num_tags)\n",
        "    outputs = crf(dense, seq_lengths)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Compile with custom loss\n",
        "    def custom_loss(y_true, y_pred):\n",
        "        return crf.crf_loss(y_true, y_pred, seq_lengths)\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=custom_loss, metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim_ner = 128\n",
        "lstm_units_ner = 64\n",
        "\n",
        "# Build NER Model\n",
        "model_ner = build_ner_model(vocab_size, embedding_dim_ner, lstm_units_ner, num_tags, max_seq_length)\n",
        "model_ner.summary()\n",
        "\n",
        "# Callbacks\n",
        "callbacks_ner = [\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath=\"best_ner_model.h5\", save_best_only=True, save_weights_only=True),\n",
        "]\n",
        "\n",
        "# Training the model\n",
        "history_ner = model_ner.fit(\n",
        "    X_train_ner,\n",
        "    y_train_ner,\n",
        "    validation_data=(X_test_ner, y_test_ner),\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks_ner,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HHAA3OoxW6ld",
        "outputId": "44765d0c-ab2f-4376-e4a0-54ef8e62a5b1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_6 (InputLayer)           [(None, 19)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)        (None, 19, 128)      98816       ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " bidirectional_5 (Bidirectional  (None, 19, 128)     98816       ['embedding_5[0][0]']            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.ne_2 (TFOpLam  (None, 19)          0           ['input_6[0][0]']                \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 19, 128)      0           ['bidirectional_5[0][0]']        \n",
            "                                                                                                  \n",
            " tf.cast_8 (TFOpLambda)         (None, 19)           0           ['tf.__operators__.ne_2[0][0]']  \n",
            "                                                                                                  \n",
            " time_distributed_3 (TimeDistri  (None, 19, 17)      2193        ['dropout_5[0][0]']              \n",
            " buted)                                                                                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_sum_2 (TFOpLamb  (None,)             0           ['tf.cast_8[0][0]']              \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " crf_layer (CRFLayer)           (None, 19)           289         ['time_distributed_3[0][0]',     \n",
            "                                                                  'tf.math.reduce_sum_2[0][0]']   \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 200,114\n",
            "Trainable params: 200,114\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "StagingError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"<ipython-input-15-980f3ff868e4>\", line 47, in custom_loss  *\n        return crf.crf_loss(y_true, y_pred, seq_lengths)\n    File \"<ipython-input-15-980f3ff868e4>\", line 18, in crf_loss  *\n        log_likelihood, _ = crf_log_likelihood(\n    File \"/usr/local/lib/python3.10/dist-packages/tensorflow_addons/text/crf.py\", line 228, in crf_log_likelihood  *\n        num_tags = inputs.shape[2]\n\n    IndexError: tuple index out of range\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStagingError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-980f3ff868e4>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m history_ner = model_ner.fit(\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mX_train_ner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my_train_ner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1267\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStagingError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"<ipython-input-15-980f3ff868e4>\", line 47, in custom_loss  *\n        return crf.crf_loss(y_true, y_pred, seq_lengths)\n    File \"<ipython-input-15-980f3ff868e4>\", line 18, in crf_loss  *\n        log_likelihood, _ = crf_log_likelihood(\n    File \"/usr/local/lib/python3.10/dist-packages/tensorflow_addons/text/crf.py\", line 228, in crf_log_likelihood  *\n        num_tags = inputs.shape[2]\n\n    IndexError: tuple index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks for training\n",
        "callbacks_ner = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath='best_model_ner.h5', save_best_only=True, save_weights_only=True)\n",
        "]\n",
        "\n",
        "# Training the model\n",
        "history_ner = model_ner.fit(\n",
        "    X_train_ner, y_train_ner,\n",
        "    validation_data=(X_test_ner, y_test_ner),\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks_ner\n",
        ")"
      ],
      "metadata": {
        "id": "zhBQC_QkW9VY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}