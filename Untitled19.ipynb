{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO+pXGKrMbPP4kdlticHjAz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanbayup/Machine-Learning/blob/main/Untitled19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.11.0\n",
        "!pip install seqeval\n",
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZQPooANmiDr",
        "outputId": "93cfa46d-02ca-4808-9c36-aa6ee535e509"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.11.0 in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.67.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.12.1)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.2)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.45.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
            "Collecting seqeval\n",
            "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=f41906617780cd55d0c203bcb99de118db677a8b703d75d9580bc053203de2de\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, TimeDistributed, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow_addons as tfa\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXGwKz_VWA1E",
        "outputId": "15c15be4-134b-4924-94be-c35af57cf69b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.11.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset from dataaa.json\n",
        "with open('dataaa.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display first few entries\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtL885VtWotz",
        "outputId": "19094b61-605f-4629-954b-982d6e9e24b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          utterances               intent  \\\n",
            "0  Anjing saya mengalami gatal-gatal terus meneru...  medical_inquiry_dog   \n",
            "1  Anjing saya terlihat sering muntah dan kehilan...  medical_inquiry_dog   \n",
            "2  Anjing saya terlihat lesu, demam, dan tidak ma...  medical_inquiry_dog   \n",
            "3  Anjing saya mengalami batuk kering dan nafasny...  medical_inquiry_dog   \n",
            "4  Anjing saya mengalami luka pada kulit yang men...  medical_inquiry_dog   \n",
            "\n",
            "                                            entities  \\\n",
            "0  [{'entity': 'animal', 'value': 'Anjing', 'star...   \n",
            "1  [{'entity': 'animal', 'value': 'Anjing', 'star...   \n",
            "2  [{'entity': 'animal', 'value': 'Anjing', 'star...   \n",
            "3  [{'entity': 'animal', 'value': 'Anjing', 'star...   \n",
            "4  [{'entity': 'animal', 'value': 'Anjing', 'star...   \n",
            "\n",
            "                                           responses  \n",
            "0  Gunakan sampo hipoalergenik, oleskan salep hid...  \n",
            "1  Berikan cairan elektrolit untuk mencegah dehid...  \n",
            "2  Pastikan anjing tetap terhidrasi, gunakan komp...  \n",
            "3  Berikan obat batuk khusus anjing yang disarank...  \n",
            "4  Bersihkan luka dengan antiseptik, oleskan sale...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text"
      ],
      "metadata": {
        "id": "PBmMeOIMWvfT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Cleaning to Utterances\n",
        "df['clean_utterances'] = df['utterances'].apply(clean_text)\n",
        "\n",
        "# Encode Intents\n",
        "label_encoder = LabelEncoder()\n",
        "df['intent_label'] = label_encoder.fit_transform(df['intent'])\n",
        "num_intent_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Prepare Entities for NER\n",
        "all_labels = set(['O'])\n",
        "for label_list in df['entities']:\n",
        "    for ent in label_list:\n",
        "        all_labels.add('B-' + ent['entity'])\n",
        "        all_labels.add('I-' + ent['entity'])\n",
        "\n",
        "# Create tag2idx and idx2tag mappings\n",
        "tag2idx = {tag: idx for idx, tag in enumerate(sorted(all_labels))}\n",
        "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
        "num_tags = len(tag2idx)\n",
        "\n",
        "# Initialize Tokenizer\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['clean_utterances'])\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1  # +1 for padding\n",
        "\n",
        "# Convert Utterances to Sequences\n",
        "sequences = tokenizer.texts_to_sequences(df['clean_utterances'])\n",
        "\n",
        "# Determine Maximum Sequence Length\n",
        "max_seq_length = max(len(seq) for seq in sequences)\n",
        "\n",
        "# Pad Sequences\n",
        "X = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Prepare NER Labels\n",
        "def prepare_ner_labels(entities, tokens, tag2idx):\n",
        "    labels = ['O'] * len(tokens)\n",
        "    for ent in entities:\n",
        "        entity = ent['entity']\n",
        "        value = ent['value']\n",
        "        # Tokenize entity value\n",
        "        ent_text_clean = clean_text(value)\n",
        "        ent_tokens = ent_text_clean.split()\n",
        "        ent_length = len(ent_tokens)\n",
        "        for i in range(len(tokens) - ent_length + 1):\n",
        "            if tokens[i:i+ent_length] == ent_tokens:\n",
        "                labels[i] = 'B-' + entity\n",
        "                for j in range(1, ent_length):\n",
        "                    labels[i + j] = 'I-' + entity\n",
        "                break  # Avoid overlapping\n",
        "    # Convert labels to indices and pad\n",
        "    label_ids = [tag2idx.get(label, tag2idx['O']) for label in labels]\n",
        "    label_ids += [tag2idx['O']] * (max_seq_length - len(label_ids))\n",
        "    return label_ids[:max_seq_length]\n",
        "\n",
        "# Apply to All Entries\n",
        "y_ner = []\n",
        "for idx, row in df.iterrows():\n",
        "    tokens = df['clean_utterances'].iloc[idx].split()\n",
        "    label_ids = prepare_ner_labels(row['entities'], tokens, tag2idx)\n",
        "    y_ner.append(label_ids)\n",
        "\n",
        "y_ner = np.array(y_ner)"
      ],
      "metadata": {
        "id": "DzdYzgNUWxuL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Data into Train and Test for Intent Classification\n",
        "X_train_intent, X_test_intent, y_train_intent, y_test_intent = train_test_split(\n",
        "    X, df['intent_label'], test_size=0.1, random_state=42, stratify=df['intent_label']\n",
        ")\n",
        "\n",
        "# Split Data into Train and Test for NER\n",
        "X_train_ner, X_test_ner, y_train_ner, y_test_ner = train_test_split(\n",
        "    X, y_ner, test_size=0.1, random_state=42, stratify=df['intent_label']\n",
        ")\n",
        "\n",
        "print(\"Intent Classification:\")\n",
        "print(\"Train:\", X_train_intent.shape, y_train_intent.shape)\n",
        "print(\"Test:\", X_test_intent.shape, y_test_intent.shape)\n",
        "\n",
        "print(\"\\nNER:\")\n",
        "print(\"Train:\", X_train_ner.shape, y_train_ner.shape)\n",
        "print(\"Test:\", X_test_ner.shape, y_test_ner.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsaT-oIeWzvJ",
        "outputId": "26059569-0361-4797-da9b-8f2ca049f926"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent Classification:\n",
            "Train: (511, 19) (511,)\n",
            "Test: (57, 19) (57,)\n",
            "\n",
            "NER:\n",
            "Train: (511, 19) (511, 19)\n",
            "Test: (57, 19) (57, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define Intent Classification Model\n",
        "def build_intent_model(vocab_size, embedding_dim, lstm_units, num_classes, max_length):\n",
        "    inputs = Input(shape=(max_length,), dtype='int32')\n",
        "    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length, mask_zero=True)(inputs)\n",
        "    bilstm = Bidirectional(LSTM(units=lstm_units))(embedding)\n",
        "    dropout = Dropout(0.5)(bilstm)\n",
        "    dense = Dense(64, activation='relu')(dropout)\n",
        "    outputs = Dense(num_classes, activation='softmax')(dense)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim_intent = 128\n",
        "lstm_units_intent = 64\n",
        "\n",
        "# Build Intent Model\n",
        "model_intent = build_intent_model(vocab_size, embedding_dim_intent, lstm_units_intent, num_intent_classes, max_seq_length)\n",
        "model_intent.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9Q9GdiGW3wY",
        "outputId": "568d030f-c292-4dec-d423-b4bb0600cd5a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 19)]              0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 19, 128)           98816     \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128)              98816     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                1040      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 206,928\n",
            "Trainable params: 206,928\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define NER Model\n",
        "def build_ner_model(vocab_size, embedding_dim, lstm_units, num_tags, max_length):\n",
        "    inputs = Input(shape=(max_length,), dtype='int32')\n",
        "    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length, mask_zero=True)(inputs)\n",
        "    bilstm = Bidirectional(LSTM(units=lstm_units, return_sequences=True))(embedding)\n",
        "    dropout = Dropout(0.5)(bilstm)\n",
        "    dense = TimeDistributed(Dense(100, activation='relu'))(dropout)\n",
        "    crf = tfa.layers.CRF(num_tags)\n",
        "    outputs = crf(dense)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer='adam', loss=crf.loss, metrics=[crf.metrics])\n",
        "    return model\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim_ner = 128\n",
        "lstm_units_ner = 64\n",
        "\n",
        "# Build NER Model\n",
        "model_ner = build_ner_model(vocab_size, embedding_dim_ner, lstm_units_ner, num_tags, max_seq_length)\n",
        "model_ner.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "HHAA3OoxW6ld",
        "outputId": "f9adfee6-252c-4da3-f833-b90b9a32005f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'CRF' object has no attribute 'loss'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ea875176f533>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Build NER Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel_ner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_ner_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim_ner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_units_ner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mmodel_ner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-ea875176f533>\u001b[0m in \u001b[0;36mbuild_ner_model\u001b[0;34m(vocab_size, embedding_dim, lstm_units, num_tags, max_length)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CRF' object has no attribute 'loss'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zhBQC_QkW9VY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}